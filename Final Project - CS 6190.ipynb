{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing neural networks with Gaussian mixture priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction and Concepts \n",
    "\n",
    "In this project we have implemented the paper titled [Soft weight-sharing for Neural Network compression](https://arxiv.org/abs/1702.04008) by Ullrich, Meeds and Welling. The main idea of the paper is to introduce a prior on the weights of a pre-trained network that encourages a lot of weights to go to zero and clusters the remaining points around a small number of discrete value. \n",
    "\n",
    "This is done by using a Gaussian Mixture prior over the weights such that the most of the weights map to a gaussian with zero mean and the rest of the weights are quantized to their closest cluster centers.\n",
    "\n",
    "The high level idea here is to compress the multi-million parameters so as to facilitate storing them on small devices like Raspberry Pis, Smartphones etc. This idea has been researched for many years now and it is still an open area of research. In this paper, author aims at pruning the neural networks by exploting the redundancy between the weights. To ensure the correctness of the algorithm, the authors are using well known MNIST dataset and a 2-fullly connected neural network. Essentially we want a distribution that has most of the mass in the zero peak and a small amount of mass on other discrete values. \n",
    "\n",
    "From the paper:\n",
    "> By fitting the mixture components alongside the weights, the weights tend to concentrate very tightly around a number of cluster components, while the cluster centers optimize themselves to give the network high predictive accuracy. Compression is achieved because we only need to encode K cluster means (in full precision) in addition to the assignment of each weight to one of these J values (using log(J) bits per weight)\n",
    "\n",
    "$\\textbf{Objective Functions}:$\n",
    "$$\\mathcal{L}(\\mathbf{w}, \\{\\mu_j, \\sigma_j, \\pi_j\\}_{j=0}^J) = \\mathcal{L}^E + \\tau\\mathcal{L}^C$$\n",
    "$$\\mathcal{L}(\\mathbf{w}, \\{\\mu_j, \\sigma_j, \\pi_j\\}_{j=0}^J) = -\\log p(\\mathbf{T | X,w}) - \\tau \\log p(\\mathbf{w}, \\{\\mu_j, \\sigma_j, \\pi_j\\}_{j=0}^J)$$\n",
    "\n",
    "These equations from the paper [1] are implemented in the code below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of Process\n",
    "\n",
    "Our code is seperated into different python files. We have used Python3.6 for the entire project development. Also, we are using keras 2.0.0 instead of the latest version due to some compatibility issues.\n",
    "\n",
    "Following are the steps to achieve compression using the methods described in the given paper:\n",
    "\n",
    "1. Retraining a pre trained network with gaussian mixture prior on the weights\n",
    "2. Clustering the weights, merging redundant components and retrain. It also involves thresholding the weight components around zero. \n",
    "3. Quantize the weights by mapping them to nearest cluster mean\n",
    "\n",
    "Here, we are training the neural network using Gausian Mixture Model (GMM) Prior. The reason is, GMM Prior being a Bayesian Prior facilitates the learning of the parameters of prior as well. This allows the weights to naturally cluster together and to lower the variance of the Gaussian Mixture which results in higher probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Retraining a Pre-Trained Neural Network \n",
    "\n",
    "Here, we take a reasonably sized neural network to compress. First, we load the dataset in the data.py file and then we train a simple 2-layer convolutional 2-fully connected network on the MNIST dataset. It has approximately 642000 parameters. We are using the tensorflow backend and some keras library functionalities.  \n",
    "\n",
    "Here, we load the MNIST (from Keras) dataset into Memory with 60K training samples and 10K test samples. For the neural network, we use ReLu activation. We are also using Adam Optimizer with 1 epoch. The test accuracy we get with this is around 97 % which increases to 99% if we increase the number of epochs to 50-60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/archit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from data import get_mnist\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,  Activation, Flatten, Conv2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 25)        650       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 5, 50)          11300     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               625500    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "_________________________________________________________________\n",
      "error_loss (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 642,460.0\n",
      "Trainable params: 642,460.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the training data, this loads the mnist dataset if not already present\n",
    "X_train, X_test, Y_train, Y_test, img_rows, img_cols, num_classes = get_mnist()\n",
    "\n",
    "# Create a data input layer\n",
    "InputLayer = Input(shape=(img_rows, img_cols,1), name=\"input\")\n",
    "\n",
    "# First convolution layer\n",
    "conv_1 = Conv2D(25, (5, 5), strides = (2,2), activation = \"relu\")(InputLayer)\n",
    "# Second convolution layer\n",
    "conv_2 = Conv2D(50, (3, 3), strides = (2,2), activation = \"relu\")(conv_1)\n",
    "\n",
    "# 2 fully connected layers with RELU activations\n",
    "conv_output = Flatten()(conv_2)\n",
    "fc1 = Dense(500)(conv_output)\n",
    "fc1 = Activation(\"relu\")(fc1)\n",
    "fc2 = Dense(num_classes)(fc1)\n",
    "PredictionLayer = Activation(\"softmax\", name =\"error_loss\")(fc2)\n",
    "\n",
    "# Fianlly, we create a model object:\n",
    "reference_model = Model(inputs=[InputLayer], outputs=[PredictionLayer])\n",
    "reference_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2550: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.8915 - acc: 0.7612 - val_loss: 0.4115 - val_acc: 0.8590\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.2947 - acc: 0.9158 - val_loss: 0.2948 - val_acc: 0.8990\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.1972 - acc: 0.9427 - val_loss: 0.1959 - val_acc: 0.9380\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.1430 - acc: 0.9581 - val_loss: 0.1360 - val_acc: 0.9580\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.1014 - acc: 0.9705 - val_loss: 0.1182 - val_acc: 0.9640\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0962 - acc: 0.9735 - val_loss: 0.1045 - val_acc: 0.9700\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0763 - acc: 0.9775 - val_loss: 0.0988 - val_acc: 0.9700\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0501 - acc: 0.9863 - val_loss: 0.0805 - val_acc: 0.9740\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0379 - acc: 0.9892 - val_loss: 0.0733 - val_acc: 0.9770\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0309 - acc: 0.9916 - val_loss: 0.0794 - val_acc: 0.9790\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0291 - acc: 0.9913 - val_loss: 0.0830 - val_acc: 0.9690\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0220 - acc: 0.9944 - val_loss: 0.0729 - val_acc: 0.9770\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0143 - acc: 0.9967 - val_loss: 0.0728 - val_acc: 0.9790\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0752 - val_acc: 0.9780\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0134 - acc: 0.9961 - val_loss: 0.0738 - val_acc: 0.9770\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0104 - acc: 0.9975 - val_loss: 0.0846 - val_acc: 0.9770\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0066 - acc: 0.9988 - val_loss: 0.0792 - val_acc: 0.9800\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0045 - acc: 0.9994 - val_loss: 0.0849 - val_acc: 0.9770\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0796 - val_acc: 0.9810\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0841 - val_acc: 0.9780\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0026 - acc: 0.9999 - val_loss: 0.0789 - val_acc: 0.9790\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0775 - val_acc: 0.9770\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0811 - val_acc: 0.9790\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 2s - loss: 8.5033e-04 - acc: 1.0000 - val_loss: 0.0830 - val_acc: 0.9790\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 2s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9780\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 2s - loss: 7.7600e-04 - acc: 1.0000 - val_loss: 0.0837 - val_acc: 0.9800\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 2s - loss: 5.7774e-04 - acc: 1.0000 - val_loss: 0.0849 - val_acc: 0.9800\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 2s - loss: 5.0006e-04 - acc: 1.0000 - val_loss: 0.0835 - val_acc: 0.9800\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 2s - loss: 4.6815e-04 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 0.9800\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 2s - loss: 4.2413e-04 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 0.9800\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 2s - loss: 3.8089e-04 - acc: 1.0000 - val_loss: 0.0872 - val_acc: 0.9800\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 2s - loss: 3.6783e-04 - acc: 1.0000 - val_loss: 0.0874 - val_acc: 0.9800\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 2s - loss: 3.2859e-04 - acc: 1.0000 - val_loss: 0.0873 - val_acc: 0.9780\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 2s - loss: 3.0471e-04 - acc: 1.0000 - val_loss: 0.0899 - val_acc: 0.9800\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 2s - loss: 3.4094e-04 - acc: 1.0000 - val_loss: 0.0896 - val_acc: 0.9810\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 2s - loss: 2.7258e-04 - acc: 1.0000 - val_loss: 0.0896 - val_acc: 0.9780\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 2s - loss: 3.2217e-04 - acc: 1.0000 - val_loss: 0.0915 - val_acc: 0.9780\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 2s - loss: 2.4318e-04 - acc: 1.0000 - val_loss: 0.0898 - val_acc: 0.9810\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 2s - loss: 2.2327e-04 - acc: 1.0000 - val_loss: 0.0905 - val_acc: 0.9800\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 2s - loss: 2.4106e-04 - acc: 1.0000 - val_loss: 0.0917 - val_acc: 0.9780\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.9254e-04 - acc: 1.0000 - val_loss: 0.0936 - val_acc: 0.9800\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.8549e-04 - acc: 1.0000 - val_loss: 0.0940 - val_acc: 0.9800\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.6898e-04 - acc: 1.0000 - val_loss: 0.0946 - val_acc: 0.9790\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.5710e-04 - acc: 1.0000 - val_loss: 0.0936 - val_acc: 0.9800\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.5095e-04 - acc: 1.0000 - val_loss: 0.0939 - val_acc: 0.9780\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.4023e-04 - acc: 1.0000 - val_loss: 0.0955 - val_acc: 0.9790\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.3423e-04 - acc: 1.0000 - val_loss: 0.0964 - val_acc: 0.9790\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.2849e-04 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9800\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.2182e-04 - acc: 1.0000 - val_loss: 0.0960 - val_acc: 0.9790\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 2s - loss: 1.2020e-04 - acc: 1.0000 - val_loss: 0.0976 - val_acc: 0.9790\n",
      "Test accuracy: 0.979\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "reference_model.compile(optimizer, loss = {\"error_loss\": \"categorical_crossentropy\"}, metrics=[\"accuracy\"])\n",
    "\n",
    "reference_model.fit(x=X_train, y=Y_train, \n",
    "          epochs= epochs, batch_size = batch_size,\n",
    "          verbose = 1, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = reference_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 ) Clustering the weights, merging redundant components and retrain. It also involves thresholding the weight components around zero. \n",
    "\n",
    "Here, first we save and load our trained model from Part 1. Then we initialize the Gaussian Mixture Model Prior with 16 components. Here, we set the pi_zero as 0.99. Then. we select a Gamma Hyper-prior on the precisions of the Gaussian Mixture prior. The variance of the hyper-prior is considered as an estimate of the extent to which the variance is regularized over the distribution. As it goes with our intuition, the variance of the zeroth component should have more weight than any other component thus we put a stronger prior on the zeroth component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.models.save_model(reference_model, \"./ref_model\")\n",
    "\n",
    "pre_trained_model = keras.models.load_model(\"./ref_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1026: calling reduce_min (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1008: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 12, 12, 25)    650                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 5, 5, 50)      11300                                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1250)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 500)           625500                                       \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            5010                                         \n",
      "____________________________________________________________________________________________________\n",
      "error_loss (Activation)          (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "complexity_loss (GMMPrior)       (None, 1)             46                                           \n",
      "====================================================================================================\n",
      "Total params: 642,506.0\n",
      "Trainable params: 642,506.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from priors import GMMPrior\n",
    "from keras_helpers import fetch_weights\n",
    "\n",
    "pi_zero = 0.99\n",
    "\n",
    "reg_layer = GMMPrior(16, fetch_weights(reference_model), \n",
    "                     pre_trained_model.get_weights(), pi_zero, name=\"complexity_loss\")(fc2)\n",
    "\n",
    "compressed_model = Model(inputs=[InputLayer], outputs=[PredictionLayer, reg_layer])\n",
    "\n",
    "compressed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import optimizers \n",
    "from keras_helpers import identity\n",
    "\n",
    "tau = 0.003\n",
    "N = X_train.shape[0] \n",
    "\n",
    "opt = optimizers.Adam(lr = [5e-4,1e-4,3e-3,3e-3], param_types_dict = ['means','gammas','rhos'])\n",
    "\n",
    "compressed_model.compile(optimizer = opt,\n",
    "              loss = {\"error_loss\": \"categorical_crossentropy\", \"complexity_loss\": identity},\n",
    "              loss_weights = {\"error_loss\": 1. , \"complexity_loss\": tau/N},\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.0953 - error_loss_loss: 0.0035 - complexity_loss_loss: -329220.7362 - error_loss_acc: 0.9991 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 2/30\n",
      "10000/10000 [==============================] - 54s - loss: -0.1043 - error_loss_loss: 0.0060 - complexity_loss_loss: -367848.0476 - error_loss_acc: 0.9984 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 3/30\n",
      "10000/10000 [==============================] - 54s - loss: -0.1204 - error_loss_loss: 0.0015 - complexity_loss_loss: -406121.3617 - error_loss_acc: 0.9999 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 4/30\n",
      "10000/10000 [==============================] - 54s - loss: -0.1328 - error_loss_loss: 4.2095e-04 - complexity_loss_loss: -444146.6849 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 5/30\n",
      "10000/10000 [==============================] - 54s - loss: -0.1443 - error_loss_loss: 2.5393e-04 - complexity_loss_loss: -481929.5982 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 6/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.1556 - error_loss_loss: 1.9483e-04 - complexity_loss_loss: -519458.7587 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 7/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.1668 - error_loss_loss: 1.6834e-04 - complexity_loss_loss: -556707.3086 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 8/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.1779 - error_loss_loss: 1.5479e-04 - complexity_loss_loss: -593653.9785 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 9/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.1889 - error_loss_loss: 1.4543e-04 - complexity_loss_loss: -630272.8085 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 10/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.1998 - error_loss_loss: 1.3880e-04 - complexity_loss_loss: -666565.2425 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 11/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.2106 - error_loss_loss: 1.3714e-04 - complexity_loss_loss: -702521.3840 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 12/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.2213 - error_loss_loss: 1.3380e-04 - complexity_loss_loss: -738157.8031 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 13/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.2319 - error_loss_loss: 1.3639e-04 - complexity_loss_loss: -773448.1269 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 14/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.2424 - error_loss_loss: 1.3877e-04 - complexity_loss_loss: -808395.5060 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 15/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.2528 - error_loss_loss: 1.4361e-04 - complexity_loss_loss: -843004.4843 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 16/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.2630 - error_loss_loss: 1.5658e-04 - complexity_loss_loss: -877303.9508 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 17/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.2732 - error_loss_loss: 1.8424e-04 - complexity_loss_loss: -911321.2498 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 18/30\n",
      "10000/10000 [==============================] - 55s - loss: -0.2833 - error_loss_loss: 2.0393e-04 - complexity_loss_loss: -945107.8207 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 19/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.2934 - error_loss_loss: 2.1723e-04 - complexity_loss_loss: -978703.6057 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 20/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.3034 - error_loss_loss: 2.4815e-04 - complexity_loss_loss: -1012205.9051 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 21/30\n",
      "10000/10000 [==============================] - 54s - loss: -0.3134 - error_loss_loss: 2.9957e-04 - complexity_loss_loss: -1045647.0588 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 22/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.3234 - error_loss_loss: 3.5359e-04 - complexity_loss_loss: -1079083.7262 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 23/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.2887 - error_loss_loss: 0.0441 - complexity_loss_loss: -1109478.7396 - error_loss_acc: 0.9866 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 24/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.3172 - error_loss_loss: 0.0223 - complexity_loss_loss: -1131468.8306 - error_loss_acc: 0.9922 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 25/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.3427 - error_loss_loss: 0.0043 - complexity_loss_loss: -1156564.9004 - error_loss_acc: 0.9997 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 26/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.3455 - error_loss_loss: 0.0090 - complexity_loss_loss: -1181649.2230 - error_loss_acc: 0.9978 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 27/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.3600 - error_loss_loss: 0.0018 - complexity_loss_loss: -1206081.7438 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 28/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.3682 - error_loss_loss: 0.0011 - complexity_loss_loss: -1230948.4718 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 29/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.3758 - error_loss_loss: 8.6729e-04 - complexity_loss_loss: -1255530.4608 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 30/30\n",
      "10000/10000 [==============================] - 53s - loss: -0.3830 - error_loss_loss: 9.4820e-04 - complexity_loss_loss: -1279813.6540 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd8ee8def98>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 256\n",
    "compressed_model.fit({\"input\": X_train,},\n",
    "          {\"error_loss\" : Y_train, \"complexity_loss\": np.zeros((N,1))},\n",
    "          epochs = epochs,\n",
    "          batch_size = batch_size,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 3 - Post Processing Steps\n",
    "\n",
    "Finally, now we have a pre-trained neural network with Gaussian Mixture priors applied to it. Now, the last step is to quantize the weights or in other words, setting up the weights according to the observed mean of different components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_retrain = np.copy(compressed_model.get_weights())\n",
    "weights_compressed = np.copy(compressed_model.get_weights())\n",
    "weights_compressed[:-3] = helper_functions.discretesize(np.copy(weights_compressed), pi_zero = pi_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next step is to compare the accuracy of the pre-trained network with the network obtained post-processing. The procedure to do that is as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[[[-1.16319485e-01,  2.30968557e-02,  2.03786697e-02,\n",
       "           8.92429948e-02,  1.77856624e-01,  1.27547950e-01,\n",
       "           1.02748618e-01, -3.80000286e-02, -3.73201221e-02,\n",
       "           1.89283013e-01, -4.64038216e-02, -7.45273903e-02,\n",
       "          -6.14067949e-02,  3.40654626e-02, -1.71312820e-02,\n",
       "           6.08386621e-02,  2.38881875e-02, -1.13360919e-01,\n",
       "           1.64433002e-01,  1.05244264e-01, -9.12514776e-02,\n",
       "           7.91243184e-03, -6.36823475e-03,  6.91204146e-03,\n",
       "          -2.32536010e-02]],\n",
       "\n",
       "        [[ 2.48939986e-03,  6.71253204e-02, -6.24226928e-02,\n",
       "           3.93063724e-02,  1.76426560e-01,  1.06370941e-01,\n",
       "           3.10187396e-02,  2.99057160e-02, -8.78875260e-04,\n",
       "           1.83038563e-01, -1.14855042e-03,  7.31127858e-02,\n",
       "           3.58531177e-02,  9.79370400e-02, -4.25991304e-02,\n",
       "          -7.49438852e-02, -1.20244296e-02, -2.25608051e-03,\n",
       "           1.53059155e-01,  6.16656132e-02, -7.58479908e-02,\n",
       "           1.23407483e-01, -1.47658020e-01,  1.52732819e-01,\n",
       "           7.22306892e-02]],\n",
       "\n",
       "        [[ 1.30671978e-01,  3.03948838e-02, -5.11313789e-02,\n",
       "           1.10072270e-01,  2.46134717e-02,  1.25412390e-01,\n",
       "          -9.06189457e-02, -1.21321619e-01, -6.62640557e-02,\n",
       "           4.79458161e-02, -7.00522065e-02,  9.41844136e-02,\n",
       "          -2.04817858e-02,  3.03420909e-02,  2.78591737e-02,\n",
       "           9.55586657e-02, -4.52452488e-02, -1.33514717e-01,\n",
       "           1.52816251e-01,  1.33969098e-01, -7.42707476e-02,\n",
       "           3.77212018e-02, -2.13644937e-01,  6.30700514e-02,\n",
       "           6.77716285e-02]],\n",
       "\n",
       "        [[ 8.94699693e-02, -3.94262793e-03, -8.81214999e-03,\n",
       "           1.51751950e-01, -1.58233747e-01,  1.41850626e-02,\n",
       "          -1.49496391e-01,  3.37598361e-02,  7.99104571e-03,\n",
       "          -2.95357276e-02,  5.69836702e-03, -7.44291618e-02,\n",
       "          -8.74891356e-02,  5.88906929e-02, -5.21152355e-02,\n",
       "           1.12558588e-01,  2.17175554e-03, -1.06805898e-01,\n",
       "           1.39048949e-01,  2.57954523e-02, -1.99053641e-02,\n",
       "           1.08995929e-01, -1.66801363e-01, -1.21462889e-01,\n",
       "           5.89134954e-02]],\n",
       "\n",
       "        [[ 4.26259972e-02, -2.78071184e-02, -1.09780505e-01,\n",
       "           6.81620315e-02, -1.21416226e-02,  1.18246026e-01,\n",
       "          -8.47764593e-03,  6.77244831e-03,  1.11506984e-01,\n",
       "          -6.75852522e-02, -1.35139987e-01,  4.26753424e-02,\n",
       "          -5.85902967e-02, -1.76060513e-01,  1.03795283e-01,\n",
       "           6.62750751e-02, -5.61009487e-03, -6.92675775e-03,\n",
       "           3.54888253e-02,  1.25010714e-01,  2.85058133e-02,\n",
       "           5.27632535e-02, -1.72884524e-01, -5.54310158e-02,\n",
       "           1.86091214e-02]]],\n",
       "\n",
       "\n",
       "       [[[-1.03374816e-01, -7.44024962e-02,  2.75791530e-02,\n",
       "           1.24075137e-01,  1.83585763e-01,  3.42845656e-02,\n",
       "           4.62219343e-02,  4.34236526e-02, -1.53825176e-03,\n",
       "           1.82934642e-01, -6.48625940e-02, -1.68056265e-02,\n",
       "           5.25005609e-02,  7.67838210e-02,  4.50033583e-02,\n",
       "          -7.91347325e-02, -9.71886069e-02, -8.59860480e-02,\n",
       "           1.10109849e-03,  5.77240773e-02, -5.64759858e-02,\n",
       "          -9.25857648e-02, -4.16538753e-02,  2.12704651e-02,\n",
       "           1.68591067e-01]],\n",
       "\n",
       "        [[-2.37884708e-02,  9.70305353e-02,  7.74820000e-02,\n",
       "           7.23118335e-02,  2.12843105e-01, -3.20505835e-02,\n",
       "           7.62279183e-02,  4.99267539e-04, -1.01938553e-01,\n",
       "           1.48527384e-01,  1.23172738e-01,  3.61388549e-02,\n",
       "           8.86792168e-02,  1.24240570e-01,  3.45954709e-02,\n",
       "          -8.27697013e-03, -9.15321484e-02, -5.24703041e-02,\n",
       "           9.76885930e-02,  1.89956039e-01, -6.66645840e-02,\n",
       "           1.48512885e-01, -8.79887715e-02,  1.37844816e-01,\n",
       "           1.30377576e-01]],\n",
       "\n",
       "        [[ 1.10790357e-01, -5.66571914e-02,  1.53809607e-01,\n",
       "           1.49363071e-01,  3.05166026e-03,  1.20020539e-01,\n",
       "           6.90199435e-02, -1.18398041e-01,  8.83851107e-03,\n",
       "          -1.01140618e-01,  1.22851908e-01, -4.10702713e-02,\n",
       "           5.45464493e-02,  1.12706736e-01,  9.65148956e-02,\n",
       "           2.59068042e-01,  3.84362638e-02, -9.10204723e-02,\n",
       "           1.63081944e-01,  1.72656715e-01, -5.92398010e-02,\n",
       "           6.50942326e-02, -7.84659088e-02, -3.47225331e-02,\n",
       "           1.50216132e-01]],\n",
       "\n",
       "        [[ 2.76297275e-02, -3.41554731e-02, -2.88953297e-02,\n",
       "           1.03912771e-01, -5.94457909e-02,  6.43271282e-02,\n",
       "           7.85492063e-02, -9.47331786e-02,  4.31712577e-03,\n",
       "          -2.49431700e-01, -7.08916560e-02,  3.32257487e-02,\n",
       "           7.29620904e-02, -8.64661708e-02,  8.91371965e-02,\n",
       "           1.77606747e-01,  9.91625786e-02,  5.08869328e-02,\n",
       "           8.10683668e-02,  6.98699877e-02,  1.44475639e-01,\n",
       "           5.07654855e-04, -1.92721948e-01, -1.67738274e-01,\n",
       "           6.72821030e-02]],\n",
       "\n",
       "        [[ 7.67995641e-02, -3.81490104e-02, -1.08694635e-01,\n",
       "           1.43635467e-01, -9.44926888e-02,  7.14088306e-02,\n",
       "          -1.16043307e-01,  9.16813873e-03,  7.02079907e-02,\n",
       "          -4.79764938e-02,  4.12731022e-02,  5.85921668e-02,\n",
       "           1.07620239e-01, -1.67676851e-01,  1.65260747e-01,\n",
       "          -6.39579818e-02,  1.29677087e-01, -1.33348890e-02,\n",
       "           1.52353540e-01,  8.71329531e-02,  1.83601975e-01,\n",
       "          -3.29801776e-02, -2.50061462e-03, -1.17029287e-01,\n",
       "           1.15582809e-01]]],\n",
       "\n",
       "\n",
       "       [[[-1.13919102e-01,  1.18270330e-01,  7.32765496e-02,\n",
       "           1.30291924e-01, -6.37236796e-03, -5.08372299e-02,\n",
       "           1.61416575e-01,  1.00068189e-01, -1.50508225e-01,\n",
       "           1.86246023e-01,  1.38707504e-01,  6.06185570e-02,\n",
       "           6.36900440e-02,  1.39761433e-01,  8.23244601e-02,\n",
       "           6.63954839e-02,  1.74500979e-04, -1.13467410e-01,\n",
       "          -2.11618794e-03,  1.12687401e-01, -1.96308449e-01,\n",
       "           5.12671806e-02,  1.74565047e-01,  1.26747161e-01,\n",
       "           1.37834251e-01]],\n",
       "\n",
       "        [[ 1.08840711e-01,  8.47645998e-02,  1.87386990e-01,\n",
       "           3.38090919e-02,  1.49313405e-01,  2.34513208e-02,\n",
       "           1.55215368e-01, -6.64316714e-02, -1.41420782e-01,\n",
       "           4.85439003e-02,  1.26019880e-01, -4.67175655e-02,\n",
       "           5.35555854e-02,  6.49910942e-02,  5.72737977e-02,\n",
       "           1.80386648e-01, -9.82511118e-02, -1.65127426e-01,\n",
       "          -5.68540841e-02,  1.78107787e-02, -8.77689719e-02,\n",
       "           3.92730981e-02,  1.12077162e-01,  1.95964754e-01,\n",
       "           1.10991225e-01]],\n",
       "\n",
       "        [[ 1.72457427e-01,  1.95984468e-02,  1.73168585e-01,\n",
       "          -2.09898390e-02,  8.03600699e-02,  7.50542581e-02,\n",
       "           1.18226022e-01, -6.05533496e-02, -3.95963490e-02,\n",
       "          -2.09233642e-01,  1.19635642e-01,  1.20480455e-01,\n",
       "          -2.89124739e-03,  1.99700221e-02, -3.01158708e-03,\n",
       "           1.88278869e-01,  4.44096550e-02, -7.55427778e-02,\n",
       "          -1.09054886e-01, -3.12546524e-03,  1.36957362e-01,\n",
       "          -2.24670172e-02,  4.81690764e-02,  8.53306055e-03,\n",
       "           1.62397429e-01]],\n",
       "\n",
       "        [[ 4.03533038e-03, -3.69591340e-02, -4.64316569e-02,\n",
       "           7.32082054e-02, -7.15715438e-02,  1.17449239e-01,\n",
       "          -1.44693386e-02,  3.42283538e-03,  4.55158800e-02,\n",
       "          -2.90605545e-01,  5.89172058e-02,  1.45251140e-01,\n",
       "           1.41778126e-01, -1.37613297e-01,  7.96808526e-02,\n",
       "           4.20847982e-02,  1.99575573e-01,  5.77605981e-03,\n",
       "          -6.05982281e-02, -2.14530602e-02,  1.89173982e-01,\n",
       "           1.69320162e-02,  4.89331372e-02, -1.25891641e-01,\n",
       "          -3.74133512e-02]],\n",
       "\n",
       "        [[-1.12830646e-01,  2.68240506e-03, -6.73596337e-02,\n",
       "           1.45136729e-01, -7.61873871e-02,  9.89324898e-02,\n",
       "          -3.44342254e-02,  1.75767854e-01,  1.39098108e-01,\n",
       "           1.13655202e-01,  1.66192818e-02, -6.32365644e-02,\n",
       "           8.36276636e-02, -2.02452391e-01,  1.10271074e-01,\n",
       "          -1.09745443e-01,  2.13253230e-01,  1.42701045e-01,\n",
       "          -3.72196510e-02,  4.42726538e-02,  1.63019255e-01,\n",
       "           3.85195464e-02,  7.01563433e-02, -1.44625053e-01,\n",
       "           2.23689377e-02]]],\n",
       "\n",
       "\n",
       "       [[[-1.07785426e-01,  2.45023537e-02,  9.69744995e-02,\n",
       "          -1.18234418e-01, -7.53926858e-02,  2.34330483e-02,\n",
       "           1.57300651e-01,  1.17776185e-01, -1.39363080e-01,\n",
       "           1.67510614e-01,  9.38885659e-02,  1.12971939e-01,\n",
       "          -5.03928065e-02,  4.64355871e-02, -2.02866327e-02,\n",
       "          -3.14798881e-03, -5.34467846e-02, -1.59104615e-01,\n",
       "          -1.00346617e-01,  3.36010829e-02, -1.20059691e-01,\n",
       "          -8.09944272e-02,  2.24105343e-02,  1.02173924e-01,\n",
       "           8.77185017e-02]],\n",
       "\n",
       "        [[ 6.41435832e-02,  8.64273384e-02,  2.23392129e-01,\n",
       "          -2.73356400e-02,  4.17722277e-02, -4.23372574e-02,\n",
       "           1.96380362e-01,  1.09077491e-01, -9.45275184e-03,\n",
       "          -6.48006424e-03,  1.35608792e-01,  6.42984137e-02,\n",
       "           9.25449207e-02,  1.48832664e-01, -9.14357826e-02,\n",
       "           1.34175771e-03, -1.61810592e-01, -9.48545039e-02,\n",
       "          -1.37164831e-01,  3.33429454e-03, -5.97400852e-02,\n",
       "          -1.00863595e-02,  1.96966261e-01,  7.12578371e-02,\n",
       "           4.71250601e-02]],\n",
       "\n",
       "        [[ 1.72783822e-01,  1.68919608e-01,  3.08527723e-02,\n",
       "          -1.15993410e-01,  9.00454298e-02,  3.38727422e-02,\n",
       "           1.28783017e-01,  4.06060405e-02,  1.35580108e-01,\n",
       "          -2.67131597e-01,  5.20692915e-02,  1.25510186e-01,\n",
       "           1.15652755e-01,  2.53571197e-02,  1.37584959e-03,\n",
       "          -3.40098632e-03,  9.50594898e-03, -3.00380625e-02,\n",
       "          -6.06925748e-02, -1.48703113e-01,  1.71950385e-01,\n",
       "          -7.63613591e-03,  9.62404832e-02,  9.01599601e-02,\n",
       "          -1.25680387e-01]],\n",
       "\n",
       "        [[-5.30052483e-02,  1.74654916e-01,  7.39303976e-02,\n",
       "          -1.73849851e-01, -4.93562333e-02,  1.90714877e-02,\n",
       "           4.57459223e-03,  1.60645545e-01,  1.42740175e-01,\n",
       "          -2.65745729e-01,  6.59791082e-02,  1.51368126e-01,\n",
       "           6.62716627e-02, -2.17631280e-01,  2.51210686e-02,\n",
       "           8.36723577e-03,  1.35390118e-01,  1.65214077e-01,\n",
       "          -1.08463921e-01, -1.37583241e-01,  1.72076792e-01,\n",
       "          -3.58078666e-02,  1.44459233e-01, -7.65717626e-02,\n",
       "          -1.32144496e-01]],\n",
       "\n",
       "        [[-1.55471325e-01,  1.14808410e-01,  8.10530782e-02,\n",
       "           1.90550219e-02, -7.60526508e-02, -9.06941220e-02,\n",
       "          -1.46273747e-02,  1.94691598e-01,  1.59887314e-01,\n",
       "           1.20978601e-01,  1.05917491e-01,  1.86206903e-02,\n",
       "           4.27232049e-02, -1.16813406e-01,  3.80985998e-02,\n",
       "          -3.28446664e-02,  4.67079058e-02,  1.68399543e-01,\n",
       "          -1.33270413e-01, -1.11450210e-01,  1.09786183e-01,\n",
       "           7.40959719e-02,  3.17909755e-02, -1.36519864e-01,\n",
       "          -1.32367775e-01]]],\n",
       "\n",
       "\n",
       "       [[[ 8.00712630e-02,  2.13211626e-02,  9.60471630e-02,\n",
       "          -2.05256790e-01, -7.92426914e-02, -7.83414170e-02,\n",
       "           3.66117842e-02,  4.79452498e-02,  7.81120583e-02,\n",
       "          -2.79710349e-03,  4.05063964e-02,  5.45656076e-03,\n",
       "          -1.96584943e-03,  7.35882446e-02, -1.23616308e-01,\n",
       "           1.13442428e-01, -1.79471090e-01,  1.21933535e-01,\n",
       "          -1.28765374e-01, -9.76059772e-03,  7.32459687e-03,\n",
       "           2.73654070e-02, -3.51910554e-02,  8.78897682e-03,\n",
       "          -3.26530896e-02]],\n",
       "\n",
       "        [[ 8.90532956e-02,  6.25736788e-02,  6.43539876e-02,\n",
       "          -2.43774503e-01, -7.54970685e-02,  3.76240611e-02,\n",
       "           8.11428353e-02,  1.31324068e-01, -3.82586569e-02,\n",
       "          -1.49901912e-01, -5.52343428e-02, -7.22276047e-02,\n",
       "           1.12112843e-01,  1.38048396e-01, -2.05578525e-02,\n",
       "           1.29722834e-01, -2.14281365e-01,  8.61517042e-02,\n",
       "          -1.69488907e-01, -1.16765596e-01, -3.45455632e-02,\n",
       "           7.62764737e-02,  4.54707928e-02,  1.13144919e-01,\n",
       "          -2.02674463e-01]],\n",
       "\n",
       "        [[ 2.30595637e-02,  1.57176435e-01,  2.98631918e-02,\n",
       "          -1.29188612e-01,  1.29574686e-01,  2.04031616e-02,\n",
       "          -5.46701886e-02,  7.36108422e-02,  3.49378865e-03,\n",
       "          -2.21981242e-01,  8.92357379e-02, -3.71152647e-02,\n",
       "           1.14630334e-01, -1.31921589e-01, -9.04696956e-02,\n",
       "           5.27421869e-02, -9.24965516e-02,  1.96988538e-01,\n",
       "           8.24153330e-03, -2.19890639e-01,  1.16261557e-01,\n",
       "          -3.80864032e-02,  7.39146993e-02,  2.05584869e-01,\n",
       "          -2.70025671e-01]],\n",
       "\n",
       "        [[ 3.25585678e-02,  1.85820594e-01,  7.40200561e-03,\n",
       "          -1.24530397e-01,  1.28291145e-01, -1.78846747e-01,\n",
       "          -8.64974335e-02, -1.38768647e-02,  1.68170705e-01,\n",
       "          -9.30486321e-02,  1.34682313e-01,  1.93156675e-02,\n",
       "          -2.67115049e-02, -1.81035325e-01,  6.21092878e-02,\n",
       "          -8.08368772e-02,  8.09025764e-02,  1.52834520e-01,\n",
       "           9.31231957e-03, -9.78162810e-02,  8.46712142e-02,\n",
       "          -5.02084661e-03,  9.55394879e-02,  1.75368488e-01,\n",
       "          -1.90649629e-01]],\n",
       "\n",
       "        [[-5.88717833e-02, -6.19336516e-02, -3.26922606e-03,\n",
       "          -1.61039993e-01,  3.39387842e-02, -1.81083366e-01,\n",
       "           1.03767060e-01,  1.24673806e-01,  1.58098564e-01,\n",
       "           1.72486469e-01,  2.83475313e-02,  3.44180949e-02,\n",
       "           7.76428208e-02, -1.91358864e-01,  2.50967834e-02,\n",
       "          -4.76738662e-02,  1.49033308e-01,  1.92470834e-01,\n",
       "          -6.61223009e-02, -1.53166890e-01, -1.37953209e-02,\n",
       "           7.45057985e-02,  1.88394785e-01, -3.17742378e-02,\n",
       "          -3.49419750e-02]]]], dtype=float32),\n",
       "       array([-0.02743245, -0.01373623,  0.00751439,  0.05494515, -0.01593086,\n",
       "        0.0276018 ,  0.00292629, -0.01246077,  0.01319013,  0.08653987,\n",
       "       -0.00873258, -0.02352692, -0.01836984,  0.04185927,  0.00779156,\n",
       "       -0.01571279,  0.01981789,  0.03648051,  0.03315933,  0.03876833,\n",
       "        0.00719355, -0.03383145,  0.04017898,  0.02507385,  0.00113121],\n",
       "      dtype=float32),\n",
       "       array([[[[ 0.07587305,  0.10888704, -0.04332231, ..., -0.02119604,\n",
       "           0.00555377, -0.02279412],\n",
       "         [-0.07723621,  0.01051981, -0.01369182, ...,  0.00445197,\n",
       "          -0.04252088,  0.06499533],\n",
       "         [-0.04952082,  0.01977414, -0.06127755, ...,  0.05033263,\n",
       "          -0.07963295,  0.15465684],\n",
       "         ...,\n",
       "         [-0.05093189,  0.01228887,  0.08321431, ..., -0.12481296,\n",
       "          -0.01585273,  0.08075355],\n",
       "         [-0.09735263, -0.01624816,  0.08417543, ...,  0.00814462,\n",
       "          -0.05527272,  0.12122026],\n",
       "         [-0.02881864, -0.04457103, -0.1467561 , ...,  0.08027873,\n",
       "           0.05897049,  0.01458683]],\n",
       "\n",
       "        [[ 0.08337728,  0.06612747, -0.06884244, ..., -0.02473254,\n",
       "           0.05082847,  0.01669835],\n",
       "         [ 0.09981576, -0.00522617,  0.07542463, ..., -0.05251586,\n",
       "          -0.01924118, -0.02503172],\n",
       "         [ 0.07548629, -0.04939018,  0.00458866, ..., -0.06187486,\n",
       "           0.03529798,  0.04299806],\n",
       "         ...,\n",
       "         [ 0.0890567 , -0.08855148,  0.06960703, ..., -0.11580341,\n",
       "           0.01132259,  0.19211   ],\n",
       "         [ 0.13895221, -0.06046386,  0.03270409, ...,  0.01963911,\n",
       "          -0.10406263,  0.114897  ],\n",
       "         [-0.00600992,  0.05111542, -0.1755642 , ..., -0.06933413,\n",
       "           0.06789499, -0.17434813]],\n",
       "\n",
       "        [[-0.03538196,  0.12123285,  0.02791594, ...,  0.09264805,\n",
       "          -0.04201372, -0.1351233 ],\n",
       "         [ 0.02970004, -0.08695003,  0.05157091, ..., -0.04458137,\n",
       "          -0.11945661,  0.0676092 ],\n",
       "         [ 0.0304413 ,  0.00924056,  0.06874743, ...,  0.08160384,\n",
       "          -0.03642207, -0.00587969],\n",
       "         ...,\n",
       "         [-0.02850815, -0.16820453, -0.10524897, ..., -0.01418541,\n",
       "           0.09238935,  0.13988133],\n",
       "         [-0.02678833, -0.1046509 , -0.0684725 , ...,  0.1227323 ,\n",
       "           0.03882549,  0.00296049],\n",
       "         [ 0.02179568,  0.06704882,  0.00812772, ..., -0.07042764,\n",
       "          -0.03421227, -0.17774683]]],\n",
       "\n",
       "\n",
       "       [[[-0.02609074,  0.01543515, -0.07759513, ..., -0.09495876,\n",
       "           0.1428381 , -0.1207042 ],\n",
       "         [ 0.04128458,  0.0224496 ,  0.08044154, ..., -0.04837873,\n",
       "           0.00257713, -0.05042117],\n",
       "         [ 0.05677192, -0.0297769 ,  0.02338807, ..., -0.00233179,\n",
       "           0.0466281 ,  0.0776955 ],\n",
       "         ...,\n",
       "         [-0.09162792, -0.03305338, -0.07522466, ..., -0.14562674,\n",
       "          -0.08861952, -0.04809683],\n",
       "         [-0.0632098 ,  0.08777039, -0.1397018 , ..., -0.08766115,\n",
       "           0.06333188, -0.00746792],\n",
       "         [-0.06866083, -0.01759177,  0.07361642, ..., -0.02379006,\n",
       "           0.0190679 ,  0.01813587]],\n",
       "\n",
       "        [[-0.03545588,  0.09873024,  0.03704754, ..., -0.09353677,\n",
       "          -0.05710148, -0.04840526],\n",
       "         [ 0.07654443, -0.0506345 , -0.02832125, ...,  0.01063136,\n",
       "           0.02263101,  0.07210611],\n",
       "         [-0.01244895,  0.015102  ,  0.03321976, ..., -0.02455874,\n",
       "          -0.07448073,  0.0361869 ],\n",
       "         ...,\n",
       "         [-0.15427287, -0.1468252 , -0.05736614, ..., -0.01135628,\n",
       "          -0.02135691,  0.17053576],\n",
       "         [ 0.06011641,  0.0242099 , -0.05929095, ..., -0.02252013,\n",
       "           0.0276161 ,  0.04825461],\n",
       "         [ 0.103299  , -0.14374383,  0.10019373, ..., -0.06294355,\n",
       "           0.09022106, -0.0154008 ]],\n",
       "\n",
       "        [[ 0.04386519, -0.02223351,  0.03158832, ...,  0.038107  ,\n",
       "           0.00796122, -0.14614806],\n",
       "         [-0.04422743,  0.01854614,  0.10540393, ..., -0.040201  ,\n",
       "          -0.05689495,  0.09984582],\n",
       "         [ 0.07895408,  0.03042762, -0.07628936, ...,  0.10145649,\n",
       "           0.04888346,  0.02125094],\n",
       "         ...,\n",
       "         [-0.02844771, -0.03982388,  0.01747122, ...,  0.02256415,\n",
       "          -0.03147436,  0.20906824],\n",
       "         [-0.09684186,  0.08711866,  0.01928642, ...,  0.12069102,\n",
       "          -0.13219641, -0.01730822],\n",
       "         [-0.04998412,  0.03170265, -0.02394508, ..., -0.05869371,\n",
       "           0.02535367, -0.06871282]]],\n",
       "\n",
       "\n",
       "       [[[ 0.01797127,  0.06179093, -0.00427911, ..., -0.07379046,\n",
       "          -0.01647913,  0.01403391],\n",
       "         [ 0.01084725, -0.02136995, -0.06948823, ..., -0.1106809 ,\n",
       "           0.08375145,  0.05687634],\n",
       "         [-0.03254412,  0.05105577,  0.07277478, ..., -0.08399364,\n",
       "          -0.04045643, -0.00516517],\n",
       "         ...,\n",
       "         [-0.11909209, -0.1498419 , -0.02133907, ..., -0.03357008,\n",
       "           0.04557948,  0.11691811],\n",
       "         [-0.04275795,  0.01694377, -0.01096419, ..., -0.12109634,\n",
       "           0.1477872 , -0.09506924],\n",
       "         [ 0.03474738,  0.00538233, -0.043356  , ..., -0.00331368,\n",
       "          -0.10225654,  0.07669773]],\n",
       "\n",
       "        [[-0.06723431,  0.12050415, -0.05274098, ..., -0.10455599,\n",
       "           0.00594625, -0.00848706],\n",
       "         [-0.07497817,  0.11055741, -0.07667683, ..., -0.12401834,\n",
       "           0.05556104,  0.1041892 ],\n",
       "         [ 0.01510069,  0.09354907,  0.01717317, ..., -0.09146387,\n",
       "           0.04008621,  0.01972326],\n",
       "         ...,\n",
       "         [ 0.04628279, -0.01589754, -0.07202889, ..., -0.10355715,\n",
       "           0.01748122,  0.16842932],\n",
       "         [ 0.07492467,  0.16564322, -0.03295441, ..., -0.08819066,\n",
       "          -0.07562097, -0.08981289],\n",
       "         [ 0.05437703,  0.00360181,  0.0109796 , ...,  0.05326236,\n",
       "          -0.12245853,  0.02022456]],\n",
       "\n",
       "        [[-0.09699418,  0.0452318 ,  0.07689535, ..., -0.00890119,\n",
       "          -0.0414128 ,  0.15228045],\n",
       "         [ 0.07102817,  0.10028556, -0.01670544, ..., -0.04068726,\n",
       "           0.0659162 ,  0.08008998],\n",
       "         [-0.0979567 ,  0.09591824,  0.04482669, ..., -0.05867305,\n",
       "           0.10324125,  0.08238593],\n",
       "         ...,\n",
       "         [-0.01676457,  0.07056206, -0.0685151 , ..., -0.0384104 ,\n",
       "          -0.05499931,  0.09598179],\n",
       "         [-0.08580634,  0.18069912, -0.00053074, ...,  0.11971878,\n",
       "          -0.01079734, -0.09856047],\n",
       "         [ 0.03414709, -0.07902675, -0.0259294 , ..., -0.00736012,\n",
       "           0.09642259,  0.02108989]]]], dtype=float32),\n",
       "       array([-0.00884134,  0.00662264,  0.00241042,  0.01744075,  0.01334858,\n",
       "        0.0145324 ,  0.00678666,  0.00843229,  0.03115422,  0.00540748,\n",
       "        0.0223389 ,  0.02833782, -0.00824967,  0.00451628, -0.00394641,\n",
       "        0.00298928,  0.00049757, -0.01461293,  0.02108745, -0.02355281,\n",
       "        0.00652331,  0.05251704, -0.03284943,  0.04906676, -0.03608122,\n",
       "        0.00587333, -0.02026374,  0.01731364, -0.01736273,  0.04431526,\n",
       "        0.01597491,  0.00427063, -0.0028554 ,  0.00637048, -0.00537126,\n",
       "        0.00822376,  0.01659898, -0.05348103, -0.01941401, -0.02043071,\n",
       "        0.0069174 ,  0.001738  , -0.01652694, -0.01759128, -0.03591758,\n",
       "       -0.01656888, -0.01499973, -0.00526661, -0.04460856,  0.01437369],\n",
       "      dtype=float32),\n",
       "       array([[ 7.73837091e-04,  1.85276510e-03, -2.00525741e-03, ...,\n",
       "         6.06270856e-04,  5.61315985e-03, -2.58891098e-03],\n",
       "       [ 3.61886946e-03, -1.05316555e-02,  7.37475231e-04, ...,\n",
       "         8.64014146e-04,  1.43447733e-02, -3.45149572e-04],\n",
       "       [-3.80415528e-04,  6.19892613e-04, -1.03244223e-04, ...,\n",
       "        -5.72847784e-05,  2.13689855e-04, -1.05953193e-04],\n",
       "       ...,\n",
       "       [ 5.81376581e-03,  7.82062882e-04,  1.17415760e-03, ...,\n",
       "        -1.26349519e-03,  1.08994460e-02,  2.09680665e-03],\n",
       "       [ 1.22525152e-02,  7.18564214e-03, -9.37733147e-03, ...,\n",
       "        -1.67618785e-02,  3.22315060e-02,  2.67508003e-04],\n",
       "       [ 3.01021393e-02, -2.61939578e-02, -1.22971153e-02, ...,\n",
       "        -2.54652952e-03, -3.33334021e-02,  7.11318851e-03]], dtype=float32),\n",
       "       array([ 1.06172347e-02,  2.17597615e-02, -3.29301157e-03,  6.86700456e-03,\n",
       "        7.83104729e-03,  1.95226595e-02,  1.63498316e-02,  1.37495864e-02,\n",
       "       -3.76993325e-03, -6.48448011e-03,  6.40149927e-03,  1.16620027e-02,\n",
       "        2.50459160e-03, -2.56732269e-03,  2.52803732e-02,  2.62052026e-02,\n",
       "        2.06758976e-02,  4.88224905e-03,  3.72318737e-02, -7.66447466e-03,\n",
       "        7.20072631e-03, -1.17736810e-03, -3.31533626e-02,  8.62093922e-03,\n",
       "       -7.69046659e-04,  7.26881390e-03,  3.75360693e-03, -9.84624960e-03,\n",
       "        6.33492041e-03,  4.79877723e-04,  3.09909950e-03,  2.05395631e-02,\n",
       "       -1.51169570e-02,  1.00434776e-02, -1.36763146e-02,  1.94306986e-03,\n",
       "        2.71194130e-02,  1.91213340e-02,  1.33806588e-02,  3.36507364e-04,\n",
       "       -2.01973468e-02,  1.63126588e-02, -4.07635787e-04,  1.29811624e-02,\n",
       "       -5.59856417e-04, -7.08642928e-03,  1.88405775e-02,  1.46916173e-02,\n",
       "       -7.34688947e-04,  1.66403106e-03, -1.04394667e-02, -9.41026025e-03,\n",
       "       -2.09965673e-03, -7.84743018e-03,  4.13078233e-04, -1.03470711e-02,\n",
       "        1.39859132e-02,  1.50871575e-02,  7.28504313e-03, -5.81765641e-03,\n",
       "        2.16936376e-02, -5.55510120e-03, -6.07960718e-03,  3.01660970e-03,\n",
       "        1.06476583e-02,  6.19558571e-03,  7.99161009e-03,  1.41952904e-02,\n",
       "        7.07912119e-03, -3.09327897e-03,  1.41410474e-02,  1.72057915e-02,\n",
       "       -5.19256853e-03,  1.14572477e-02,  1.08820694e-02,  9.81283840e-03,\n",
       "        4.44066664e-03, -1.71969291e-02,  1.27767734e-02,  1.13308365e-02,\n",
       "        1.20159835e-02,  3.24202864e-03,  1.90288797e-02,  1.78225199e-03,\n",
       "        1.62785221e-02,  1.50202755e-02, -2.58644141e-04, -1.61732873e-03,\n",
       "       -1.36443926e-03,  2.19977405e-02, -7.74170971e-04,  1.31481607e-02,\n",
       "        2.74522919e-02,  8.10308196e-03, -5.41683845e-03,  5.40342927e-03,\n",
       "        1.37611842e-02,  1.38159897e-02, -5.54226851e-03,  1.18573233e-02,\n",
       "        1.63123012e-02,  9.16088559e-03,  3.62533348e-04,  1.46309137e-02,\n",
       "        9.31451470e-03,  7.67983124e-03,  7.51663093e-03,  9.28988680e-03,\n",
       "        5.32154366e-03,  1.14437984e-02,  2.48263516e-02,  1.98021829e-02,\n",
       "       -1.11008529e-02,  5.69705665e-03,  2.52514519e-02, -1.76446908e-03,\n",
       "        2.83314777e-03,  1.51254395e-02,  3.22108306e-02,  9.88182053e-03,\n",
       "       -2.23941859e-02,  2.74369551e-04, -1.14018063e-03,  1.67045146e-02,\n",
       "       -1.24948733e-02,  1.58532746e-02,  2.72247382e-02,  2.94624977e-02,\n",
       "        1.78952701e-02, -5.71339345e-03,  1.09063811e-04,  7.32628256e-03,\n",
       "        1.01938949e-03, -7.03067053e-03, -1.27795688e-03,  4.78752283e-03,\n",
       "       -3.47957085e-03, -2.10918263e-02,  1.25964358e-03,  2.04248093e-02,\n",
       "       -8.60200264e-03, -7.83313625e-03,  1.34689873e-02,  1.03990631e-02,\n",
       "        2.02395786e-02, -8.53157882e-03,  1.09189693e-02, -2.66961229e-04,\n",
       "        8.03955831e-03,  7.40319444e-03, -5.29553741e-03,  4.46376903e-03,\n",
       "        1.26151252e-03, -1.30612738e-02,  8.70554533e-04, -1.61365373e-03,\n",
       "        4.61581023e-03,  1.85079668e-02,  1.12921540e-02,  1.04644801e-02,\n",
       "       -2.54950079e-04, -1.12060141e-02,  1.54683611e-03, -3.29028745e-03,\n",
       "       -1.52068734e-02,  1.22296643e-02,  1.86056010e-02, -4.73474152e-03,\n",
       "        2.82184929e-02,  2.24915780e-02,  1.60957370e-02,  1.13435443e-02,\n",
       "       -1.30985109e-02, -1.01056707e-03, -2.66835140e-03,  2.18903907e-02,\n",
       "        1.05657391e-02,  8.13703425e-03,  6.25223760e-03,  1.76587887e-02,\n",
       "        5.39093791e-03,  4.92464192e-03,  2.18446529e-03, -5.63410902e-03,\n",
       "        4.65436652e-03, -8.05968419e-04,  5.25723305e-03, -5.79813169e-03,\n",
       "        1.64436307e-02,  3.17869848e-03, -3.24430293e-03,  1.09279845e-02,\n",
       "        1.35539081e-02, -1.02708703e-02,  1.12341368e-03, -2.79272295e-04,\n",
       "        1.67066604e-02,  1.16383284e-03,  2.10755575e-03,  6.52773480e-04,\n",
       "        2.29678601e-02,  1.14773791e-02,  1.41587481e-02,  8.82777013e-03,\n",
       "        3.89893469e-03, -2.33840244e-03, -7.54054356e-03,  7.66493147e-04,\n",
       "        2.16784235e-02, -6.60319766e-03,  1.52677735e-02,  7.86739867e-03,\n",
       "        4.58097644e-03,  1.15940403e-02, -2.38746963e-03,  4.52227332e-03,\n",
       "       -3.88754088e-05,  2.19421480e-02, -8.01292527e-03,  1.30222961e-02,\n",
       "        1.41753964e-02,  2.28648391e-02,  6.36792835e-03, -3.72210355e-03,\n",
       "       -2.01621167e-02,  1.77930612e-02, -1.69441046e-03,  1.27542811e-02,\n",
       "        7.73616182e-03, -2.60217069e-03, -2.34459843e-02, -6.63244165e-03,\n",
       "        3.32514686e-03, -9.34279338e-03,  1.44867972e-02,  1.59658436e-02,\n",
       "        9.36247595e-03, -7.07494235e-03, -7.81938992e-03,  1.35308993e-03,\n",
       "        1.60746612e-02, -6.46891305e-03, -2.49625393e-03,  1.67597756e-02,\n",
       "        1.39066307e-02,  4.81590582e-03,  1.07058212e-02,  1.15296403e-02,\n",
       "        5.08548506e-03,  1.07167484e-02,  3.85572494e-04, -1.96375791e-02,\n",
       "       -2.16432270e-02,  1.74894352e-02,  1.85538139e-02,  5.35371201e-03,\n",
       "        1.71605777e-03, -7.37163145e-03, -4.23328625e-03, -2.44654762e-03,\n",
       "        1.27237588e-02,  3.79490457e-03,  1.43446298e-02,  1.42941428e-02,\n",
       "       -1.79049757e-03, -8.26124218e-04,  1.91354640e-02,  2.31464952e-02,\n",
       "        1.71669889e-02,  2.81758374e-04,  1.18963057e-02,  1.67488120e-02,\n",
       "        2.09639147e-02,  1.30633377e-02,  5.48809068e-03,  1.33581478e-02,\n",
       "       -2.95695406e-03, -2.42903596e-03,  4.08245390e-03, -2.66932487e-03,\n",
       "        1.68842990e-02, -1.18142683e-02,  1.69259626e-02,  7.48655060e-03,\n",
       "        3.75355012e-03,  8.35946389e-03,  4.72466415e-03,  2.64578741e-02,\n",
       "        1.54320169e-02,  5.35123609e-03, -1.02669410e-02, -9.52656672e-04,\n",
       "        7.13983458e-03,  1.19199716e-02,  1.12283416e-02,  2.73640174e-03,\n",
       "        1.22344987e-02, -9.25054308e-03,  2.04437766e-02,  2.27919969e-04,\n",
       "       -2.12346972e-03,  4.51774511e-04, -1.54709758e-03,  1.88636258e-02,\n",
       "       -4.86065354e-03,  9.77763440e-03,  4.45159618e-03,  2.23661563e-03,\n",
       "        1.06606130e-02,  7.69943884e-03, -3.64452600e-03,  3.26110888e-03,\n",
       "       -2.95901718e-03, -4.70140576e-03,  1.07990699e-02,  7.20264390e-03,\n",
       "        4.42835642e-03,  2.21098606e-02, -2.40942885e-04,  1.39010949e-02,\n",
       "        2.35323738e-02,  1.01047456e-02, -4.04876610e-03, -1.77429582e-03,\n",
       "       -3.05654341e-03, -9.95357893e-03,  1.53733185e-02,  1.81743875e-02,\n",
       "        2.24178191e-02, -5.05889417e-04,  1.35418877e-03, -4.96519776e-03,\n",
       "        2.94688204e-03,  1.40411640e-02,  1.00282300e-02, -4.96168737e-04,\n",
       "        8.57454259e-03,  1.61817018e-02, -7.62040354e-03,  1.25171263e-02,\n",
       "        3.56471888e-03,  5.97501546e-03,  9.29053221e-03,  5.04165282e-03,\n",
       "        1.27929086e-02,  4.54709353e-03,  2.07519177e-02,  1.84943546e-02,\n",
       "       -1.01399068e-02, -2.85894121e-03, -9.67953092e-05, -6.17963448e-03,\n",
       "        1.91354062e-02, -2.14848015e-03,  1.05793728e-02, -6.59647724e-03,\n",
       "        5.29084937e-04,  1.22132357e-02,  7.04542594e-03,  9.14870203e-03,\n",
       "        4.16031014e-03, -1.56689659e-02, -2.29314528e-03,  5.24314633e-03,\n",
       "        1.16243288e-02, -3.38810706e-03,  3.63429589e-03,  7.15199905e-03,\n",
       "        7.14854104e-03,  1.65074486e-02, -1.05719909e-03, -5.36495238e-04,\n",
       "       -5.30251942e-04,  6.83103967e-03,  8.63117352e-03,  9.08130594e-03,\n",
       "        1.51738524e-02,  1.18452096e-02, -4.79693437e-04,  3.00233532e-03,\n",
       "        1.44218393e-02,  1.04655325e-02,  2.13212352e-02,  3.02336924e-03,\n",
       "        1.00830272e-02,  1.33507233e-02,  3.78730474e-03,  8.46325886e-03,\n",
       "       -3.74412956e-03,  1.58164483e-02, -1.53332599e-03,  1.10328998e-02,\n",
       "        2.42835935e-03,  2.65626498e-02, -2.07046396e-03,  1.01076597e-02,\n",
       "       -1.29184526e-04, -1.63522403e-04,  2.57541277e-02,  4.47099889e-03,\n",
       "       -6.48957072e-03, -2.78188381e-03, -2.90178024e-04, -1.85916670e-05,\n",
       "        4.50512380e-05, -1.81390916e-03,  1.46693848e-02,  7.58170476e-03,\n",
       "        1.06532173e-02,  8.88414029e-03, -1.82553940e-02,  1.92869511e-02,\n",
       "       -6.68151863e-03, -1.31199220e-02, -6.39365008e-03,  3.73531468e-02,\n",
       "        1.94625296e-02,  1.76213821e-03,  2.24730801e-02,  2.30360106e-02,\n",
       "        7.69932987e-04,  1.58553980e-02, -4.39709765e-05,  2.30528670e-03,\n",
       "        1.74303949e-02,  1.78765096e-02,  1.22995833e-02,  1.09428568e-02,\n",
       "       -3.64696869e-04,  3.84649038e-02, -1.32084694e-02,  1.65917783e-03,\n",
       "        8.88595730e-03,  1.45473343e-03, -1.03890402e-02,  8.55867285e-03,\n",
       "        1.50598669e-02,  1.17601324e-02,  1.25123905e-02,  9.68233030e-03,\n",
       "       -1.70313602e-03, -6.79719960e-03, -9.86265135e-04,  6.56921556e-03,\n",
       "        1.12323919e-02,  1.61882993e-02, -1.18418806e-03,  1.51972137e-02,\n",
       "        2.71253521e-03, -7.80021166e-03,  1.54768368e-02,  2.13115439e-02,\n",
       "        6.76457956e-03,  2.03746986e-02,  7.02182576e-03, -2.93055637e-04,\n",
       "       -2.96734343e-03,  1.65613219e-02, -1.65624395e-02, -9.99410357e-03,\n",
       "        7.85957929e-03,  1.58351324e-02,  2.09314842e-02,  6.98928721e-03,\n",
       "       -9.60490014e-03,  1.82542559e-02,  3.92386224e-04,  9.75207426e-03,\n",
       "        7.49701587e-03,  2.54372209e-02, -1.19316195e-04, -4.73291194e-03,\n",
       "        2.16917284e-02,  1.27339661e-02,  1.87083008e-03,  7.29401736e-03,\n",
       "        5.25360228e-03,  1.64828058e-02,  8.88199080e-03,  7.98139814e-03,\n",
       "        1.45035088e-02, -4.55931621e-03,  3.82670248e-03,  1.50292451e-02,\n",
       "        1.03022046e-02,  9.37294122e-03,  1.05449241e-02,  6.97919284e-04,\n",
       "        1.42046148e-02,  8.06622300e-03,  1.05085922e-02,  1.40469205e-02,\n",
       "        1.60322897e-02, -5.16386563e-03,  4.13246802e-04,  4.52043751e-04,\n",
       "        2.87662912e-02,  3.48006724e-03, -3.24343820e-03,  1.38745252e-02],\n",
       "      dtype=float32),\n",
       "       array([[-0.11632437,  0.0291598 , -0.05362905, ...,  0.05838146,\n",
       "        -0.0599292 ,  0.05118966],\n",
       "       [ 0.01652944, -0.04477336,  0.08108103, ..., -0.1130544 ,\n",
       "         0.09775048,  0.05391113],\n",
       "       [ 0.04684921,  0.11337349,  0.02784897, ...,  0.05943854,\n",
       "        -0.15596773,  0.15610336],\n",
       "       ...,\n",
       "       [-0.05266982,  0.01728408, -0.06026829, ..., -0.03577906,\n",
       "        -0.08228491,  0.00025748],\n",
       "       [-0.13600318, -0.08912013,  0.09852294, ...,  0.1124313 ,\n",
       "        -0.07609064,  0.05541895],\n",
       "       [-0.00031722,  0.06792892, -0.04521891, ..., -0.15774518,\n",
       "        -0.12006658,  0.06570156]], dtype=float32),\n",
       "       array([-0.01638738, -0.00809453, -0.00243548, -0.00532078, -0.00205512,\n",
       "        0.00430838, -0.01054164, -0.01342964,  0.0171298 ,  0.01849291],\n",
       "      dtype=float32),\n",
       "       array([-0.58502346, -0.49530426, -0.3988792 , -0.2975053 , -0.2209673 ,\n",
       "       -0.08664611,  0.00086827,  0.00129011,  0.00079933,  0.08314122,\n",
       "        0.217584  ,  0.30347237,  0.4012725 ,  0.49572212,  0.58493555],\n",
       "      dtype=float32),\n",
       "       array([6.085396 , 6.36374  , 6.440977 , 6.5061374, 6.4865737, 6.364481 ,\n",
       "       5.9138494, 7.138666 , 6.6599913, 7.210425 , 5.9071274, 6.347741 ,\n",
       "       6.483126 , 6.5082426, 6.4448485, 6.3667088], dtype=float32),\n",
       "       array([-9.379736 , -9.436459 , -9.547411 , -9.706911 , -9.543898 ,\n",
       "       -8.505424 , -4.1751738, -5.2821383, -3.9586892, -8.427295 ,\n",
       "       -9.604901 , -9.782509 , -9.563693 , -9.440289 , -9.381291 ],\n",
       "      dtype=float32)], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"The accuracy of model is: \\n\")\n",
    "\n",
    "acc = pre_trained_model.evaluate({'input':X_test,}, {\"error_loss\": Y_test,}, verbose=0)[1]\n",
    "print(\"Reference Network: %.4f \\n\" % acc)\n",
    "\n",
    "acc2 = compressed_model.evaluate({'input': X_test,}, {\"error_loss\": Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Re-trained Network: %.4f \\n\" % acc2)\n",
    "\n",
    "compressed_model.set_weights(weights_compressed)\n",
    "\n",
    "acc3 = compressed_model.evaluate({'input': X_test,}, {\"error_loss\": Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Post Processed Network: %.4f \\n\" % acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check the number of weights that were pruned, we do the following procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Non-Zero Weights: 99.976 %\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import special_flatten as flatten_1\n",
    "weight_vec = flatten_1(weights_compressed[:-3]).flatten()\n",
    "print(\"Percentage of Non-Zero Weights: %.3f %%\" % (10.*(np.count_nonzero(weight_vec)/ weight_vec.size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8edc9f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8ec407c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8b376e400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper_functions import save_histogram\n",
    "save_histogram(pre_trained_model.get_weights(), save=\"Figures/reference\")\n",
    "save_histogram(weights_retrain, save=\"Figures/retrain\")\n",
    "save_histogram(weights_compressed, save=\"Figures/Post-Processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Resultant Images and Discussions\n",
    "\n",
    "|Weight distribution before retraining | Weight distribution after retraining|  Weight distribution after post-processing  \n",
    ":-------------------------:|:-------------------------:|:------------:|:-------------------------:\n",
    "histogram|![](./Figures/reference.png)|<img src=\"./Figures/retrain.png\"/>| ![](./Figures/Post-Processing.png)\n",
    "log-scaled histogram|![](./Figures/reference_log.png)|<img src=\"./Figures/retrain_log.png\"/>| ![](./Figures/Post-Processing_log.png)\n",
    "\n",
    "The histograms above supports the claims of the paper. We have shown the results for the histograms and log-scaled histograms. Here, the difference between the weight distributions before and after retraining is evident from the first two columns. Similarly, in the third column i.e. after performing Post-Processing, we can see that the non-zero weights are much less as compared to the original model. We also calculate the percentage of non-zero weights and it's only 10% of the orignal number of components which demonstrates the compression of neural network to a good extent. We are also running our optimizer and train the network for 50 epochs, it takes a bit of time because of large number of parameters. \n",
    "All the images attached here can be found in the Figures Folder of the submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Karen Ullrich, Edward Meeds, and Max Welling. Soft Weight-Sharing for Neural Network Compression 2017. eprint:arXiv:1702.04008. url: https://arxiv.org/abs/1702.04008\n",
    "\n",
    "[2] [Keras Tutorial on CNNs](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py)\n",
    "\n",
    "[3] Multiple codes from Keras.io - https://keras.io/\n",
    "\n",
    "[4] Tutorials by Karen Ullrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
