{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing neural networks with Gaussian mixture priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction and Concepts \n",
    "\n",
    "In this project we have implemented the paper titled [Soft weight-sharing for Neural Network compression](https://arxiv.org/abs/1702.04008) by Ullrich, Meeds and Welling. The main idea of the paper is to introduce a prior on the weights of a pre-trained network that encourages a lot of weights to go to zero and clusters the remaining points around a small number of discrete value. \n",
    "\n",
    "This is done by using a Gaussian Mixture prior over the weights such that the most of the weights map to a gaussian with zero mean and the rest of the weights are quantized to their closest cluster centers.\n",
    "\n",
    "The high level idea here is to compress the multi-million parameters so as to facilitate storing them on small devices like Raspberry Pis, Smartphones etc. This idea has been researched for many years now and it is still an open area of research. In this paper, author aims at pruning the neural networks by exploting the redundancy between the weights. To ensure the correctness of the algorithm, the authors are using well known MNIST dataset and a 2-fullly connected neural network. Essentially we want a distribution that has most of the mass in the zero peak and a small amount of mass on other discrete values. \n",
    "\n",
    "From the paper:\n",
    "> By fitting the mixture components alongside the weights, the weights tend to concentrate very tightly around a number of cluster components, while the cluster centers optimize themselves to give the network high predictive accuracy. Compression is achieved because we only need to encode K cluster means (in full precision) in addition to the assignment of each weight to one of these J values (using log(J) bits per weight)\n",
    "\n",
    "$\\textbf{Objective Functions}:$\n",
    "$$\\mathcal{L}(\\mathbf{w}, \\{\\mu_j, \\sigma_j, \\pi_j\\}_{j=0}^J) = \\mathcal{L}^E + \\tau\\mathcal{L}^C$$\n",
    "$$\\mathcal{L}(\\mathbf{w}, \\{\\mu_j, \\sigma_j, \\pi_j\\}_{j=0}^J) = -\\log p(\\mathbf{T | X,w}) - \\tau \\log p(\\mathbf{w}, \\{\\mu_j, \\sigma_j, \\pi_j\\}_{j=0}^J)$$\n",
    "\n",
    "Here $T$ is the output label, $X$ is the data, and $\\mu, \\sigma, \\pi$ are the means, variances and mixing proportions of the cluster centeres respectively.\n",
    "\n",
    "These equations from the paper [1] are implemented in the code below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of Process\n",
    "\n",
    "Our code is seperated into different python files. We have used Python3.6 for the entire project development. Also, we are using keras 2.0.0 instead of the latest version due to some compatibility issues.\n",
    "\n",
    "Following are the steps to achieve compression using the methods described in the given paper:\n",
    "\n",
    "1. Retraining a pre trained network with gaussian mixture prior on the weights\n",
    "2. Clustering the weights, merging redundant components and retrain. It also involves thresholding the weight components around zero. \n",
    "3. Quantize the weights by mapping them to nearest cluster mean\n",
    "\n",
    "Here, we are training the neural network using Gausian Mixture Model (GMM) Prior. The reason is, GMM Prior being a Bayesian Prior facilitates the learning of the parameters of prior as well. This allows the weights to naturally cluster together and to lower the variance of the Gaussian Mixture which results in higher probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Retraining a Pre-Trained Neural Network \n",
    "\n",
    "Here, we take a reasonably sized neural network to compress. First, we load the dataset in the data.py file and then we train a simple 2-layer convolutional 2-fully connected network on the MNIST dataset. It has approximately 642000 parameters. We are using the tensorflow backend and some keras library functionalities.  \n",
    "\n",
    "Here, we load the MNIST (from Keras) dataset into Memory with 60K training samples and 10K test samples. For the neural network, we use ReLu activation. We are also using Adam Optimizer with 1 epoch. The test accuracy we get with this is around 97 % which increases to 99% if we increase the number of epochs to 50-60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/archit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from data import get_mnist\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,  Activation, Flatten, Conv2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 25)        650       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 5, 50)          11300     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               625500    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "_________________________________________________________________\n",
      "error_loss (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 642,460.0\n",
      "Trainable params: 642,460.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the training data, this loads the mnist dataset if not already present\n",
    "X_train, X_test, Y_train, Y_test, img_rows, img_cols, num_classes = get_mnist()\n",
    "\n",
    "# Create a data input layer\n",
    "InputLayer = Input(shape=(img_rows, img_cols,1), name=\"input\")\n",
    "\n",
    "# First convolution layer\n",
    "conv_1 = Conv2D(25, (5, 5), strides = (2,2), activation = \"relu\")(InputLayer)\n",
    "# Second convolution layer\n",
    "conv_2 = Conv2D(50, (3, 3), strides = (2,2), activation = \"relu\")(conv_1)\n",
    "\n",
    "# 2 fully connected layers with RELU activations\n",
    "conv_output = Flatten()(conv_2)\n",
    "fc1 = Dense(500)(conv_output)\n",
    "fc1 = Activation(\"relu\")(fc1)\n",
    "fc2 = Dense(num_classes)(fc1)\n",
    "PredictionLayer = Activation(\"softmax\", name =\"error_loss\")(fc2)\n",
    "\n",
    "# Fianlly, we create a model object:\n",
    "reference_model = Model(inputs=[InputLayer], outputs=[PredictionLayer])\n",
    "reference_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2550: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Test accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "reference_model.compile(optimizer, loss = {\"error_loss\": \"categorical_crossentropy\"}, metrics=[\"accuracy\"])\n",
    "\n",
    "reference_model.fit(x=X_train, y=Y_train, \n",
    "          epochs= epochs, batch_size = batch_size,\n",
    "          verbose = 0, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = reference_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 ) Clustering the weights, merging redundant components and retrain. It also involves thresholding the weight components around zero. \n",
    "\n",
    "Here, first we save and load our trained model from Part 1. Then we initialize the Gaussian Mixture Model Prior with 16 components. Here, we set the pi_zero as 0.99. Then. we select a Gamma Hyper-prior on the precisions of the Gaussian Mixture prior. The variance of the hyper-prior is considered as an estimate of the extent to which the variance is regularized over the distribution. As it goes with our intuition, the variance of the zeroth component should have more weight than any other component thus we put a stronger prior on the zeroth component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(reference_model, \"./ref_model\")\n",
    "\n",
    "pre_trained_model = keras.models.load_model(\"./ref_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1026: calling reduce_min (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1008: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 12, 12, 25)    650                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 5, 5, 50)      11300                                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1250)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 500)           625500                                       \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            5010                                         \n",
      "____________________________________________________________________________________________________\n",
      "error_loss (Activation)          (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "complexity_loss (GMMPrior)       (None, 1)             46                                           \n",
      "====================================================================================================\n",
      "Total params: 642,506.0\n",
      "Trainable params: 642,506.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from priors import GMMPrior\n",
    "from keras_helpers import fetch_weights\n",
    "\n",
    "pi_zero = 0.99\n",
    "\n",
    "reg_layer = GMMPrior(16, fetch_weights(reference_model), \n",
    "                     pre_trained_model.get_weights(), pi_zero, name=\"complexity_loss\")(fc2)\n",
    "\n",
    "compressed_model = Model(inputs=[InputLayer], outputs=[PredictionLayer, reg_layer])\n",
    "\n",
    "compressed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimizers \n",
    "from keras_helpers import identity\n",
    "\n",
    "tau = 0.003\n",
    "N = X_train.shape[0] \n",
    "\n",
    "opt = optimizers.Adam(lr = [5e-4,1e-4,3e-3,3e-3], param_types_dict = ['means','gammas','rhos'])\n",
    "\n",
    "compressed_model.compile(optimizer = opt,\n",
    "              loss = {\"error_loss\": \"categorical_crossentropy\", \"complexity_loss\": identity},\n",
    "              loss_weights = {\"error_loss\": 1. , \"complexity_loss\": tau/N},\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 64s - loss: -0.0937 - error_loss_loss: 0.0052 - complexity_loss_loss: -329623.0506 - error_loss_acc: 0.9990 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 57s - loss: -0.1073 - error_loss_loss: 0.0032 - complexity_loss_loss: -368316.0487 - error_loss_acc: 0.9994 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 58s - loss: -0.1191 - error_loss_loss: 0.0029 - complexity_loss_loss: -406714.5803 - error_loss_acc: 0.9993 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 57s - loss: -0.1304 - error_loss_loss: 0.0030 - complexity_loss_loss: -444785.4880 - error_loss_acc: 0.9994 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 58s - loss: -0.1433 - error_loss_loss: 0.0015 - complexity_loss_loss: -482558.9606 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 63s - loss: -0.1552 - error_loss_loss: 8.4551e-04 - complexity_loss_loss: -520126.2291 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 57s - loss: -0.1668 - error_loss_loss: 4.2461e-04 - complexity_loss_loss: -557444.6958 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 57s - loss: -0.1780 - error_loss_loss: 3.2905e-04 - complexity_loss_loss: -594490.7414 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 60s - loss: -0.1891 - error_loss_loss: 2.9079e-04 - complexity_loss_loss: -631244.0984 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 55s - loss: -0.2000 - error_loss_loss: 2.8062e-04 - complexity_loss_loss: -667703.8790 - error_loss_acc: 1.0000 - complexity_loss_acc: 0.0000e+00    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3482250e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 256\n",
    "compressed_model.fit({\"input\": X_train,},\n",
    "          {\"error_loss\" : Y_train, \"complexity_loss\": np.zeros((N,1))},\n",
    "          epochs = epochs,\n",
    "          batch_size = batch_size,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 3 - Post Processing Steps\n",
    "\n",
    "Finally, now we have a pre-trained neural network with Gaussian Mixture priors applied to it. Now, the last step is to quantize the weights or in other words, setting up the weights according to the observed mean of different components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_retrain = np.copy(compressed_model.get_weights())\n",
    "weights_compressed = np.copy(compressed_model.get_weights())\n",
    "weights_compressed[:-3] = helper_functions.discretesize(np.copy(weights_compressed), pi_zero = pi_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next step is to compare the accuracy of the pre-trained network with the network obtained post-processing. The procedure to do that is as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[[[ 8.75237063e-02,  6.67930990e-02, -4.81161438e-02,\n",
       "           2.22543284e-01, -1.98470596e-02,  1.95989572e-02,\n",
       "           7.56496638e-02,  7.31949578e-04, -2.16727424e-02,\n",
       "          -4.68375906e-02,  1.46471011e-03,  7.72656947e-02,\n",
       "           6.67409375e-02,  7.57690966e-02, -9.34782904e-03,\n",
       "           1.24653697e-01, -5.16852215e-02,  1.20575286e-01,\n",
       "          -2.17459910e-02,  1.33932680e-01, -3.82486321e-02,\n",
       "          -6.08356446e-02,  3.01679503e-02,  5.62592186e-02,\n",
       "           1.60629917e-02]],\n",
       "\n",
       "        [[ 9.42064375e-02,  1.27755433e-01, -4.63198349e-02,\n",
       "           1.25586346e-01,  2.07720902e-02, -2.20584739e-02,\n",
       "          -1.33085446e-02,  4.17871661e-02, -1.13063224e-01,\n",
       "           6.30858541e-02,  1.83851179e-02, -1.70332700e-01,\n",
       "           9.91623253e-02,  3.86852771e-02, -1.88748986e-01,\n",
       "           5.84538206e-02,  1.36217013e-01, -7.08169267e-02,\n",
       "          -1.79935619e-01,  4.53407317e-02, -2.75578201e-02,\n",
       "           1.59988515e-02,  1.73083499e-01,  8.04050788e-02,\n",
       "          -5.90140326e-03]],\n",
       "\n",
       "        [[ 4.37744632e-02,  1.34269372e-01,  3.92295122e-02,\n",
       "           7.74129629e-02, -1.08351931e-01,  2.67340714e-04,\n",
       "           8.86472613e-02,  3.00868340e-02, -1.51304990e-01,\n",
       "          -1.16590271e-02,  2.34308764e-02,  1.67677756e-02,\n",
       "          -5.83803561e-03,  5.48662804e-02, -7.37753585e-02,\n",
       "          -1.07443675e-01,  1.74340717e-02, -8.35828558e-02,\n",
       "          -1.80442154e-01,  5.57726510e-02, -3.84158492e-02,\n",
       "           3.65285613e-02,  5.32102138e-02,  2.55674170e-03,\n",
       "           1.15655601e-01]],\n",
       "\n",
       "        [[-9.29991622e-03,  1.74636953e-02, -3.15655880e-02,\n",
       "           1.71190351e-01, -5.54320291e-02, -9.33894068e-02,\n",
       "          -8.48913863e-02,  4.51067928e-03, -1.71096608e-01,\n",
       "          -5.20333312e-02,  1.05621457e-01, -1.49420165e-02,\n",
       "          -5.15738614e-02, -2.14029364e-02, -1.92855835e-01,\n",
       "          -2.02464256e-02,  4.86386828e-02, -1.52475864e-01,\n",
       "          -3.85583118e-02,  1.49635255e-01,  4.42567319e-02,\n",
       "           5.07902242e-02, -3.21874544e-02, -7.11109936e-02,\n",
       "           7.34581659e-03]],\n",
       "\n",
       "        [[-4.86462712e-02,  7.85958543e-02,  1.03476465e-01,\n",
       "           8.22095647e-02, -2.73906738e-02, -4.81013805e-02,\n",
       "          -8.03400800e-02, -1.42720332e-02, -1.51403055e-01,\n",
       "           2.67704334e-02,  1.14061452e-01, -3.04281581e-02,\n",
       "           1.10547386e-01,  1.29109398e-01, -6.33557290e-02,\n",
       "          -4.23583053e-02, -1.36649027e-01, -1.17992006e-01,\n",
       "          -1.15302399e-01,  1.13680720e-01,  1.69433039e-02,\n",
       "           7.11211637e-02, -3.03788520e-02, -1.92840442e-01,\n",
       "          -8.34488049e-02]]],\n",
       "\n",
       "\n",
       "       [[[-8.62245858e-02,  9.60971862e-02, -1.64615631e-01,\n",
       "           5.86041547e-02,  5.02268150e-02, -2.71786340e-02,\n",
       "          -5.11829555e-02, -2.14578211e-02,  3.74022052e-02,\n",
       "          -1.14015639e-01,  1.18280854e-02, -3.44780162e-02,\n",
       "           9.82564911e-02,  1.33556902e-01,  6.18563294e-02,\n",
       "          -6.09212974e-03, -7.02694580e-02,  6.20449297e-02,\n",
       "          -4.81438003e-02,  9.45995450e-02,  3.93995941e-02,\n",
       "          -1.53856963e-01,  1.72089964e-01,  2.09837213e-01,\n",
       "           1.58691496e-01]],\n",
       "\n",
       "        [[-5.91026060e-02,  1.08677126e-01, -1.61374673e-01,\n",
       "           7.78420046e-02, -9.30750668e-02, -5.32057770e-02,\n",
       "           7.66901858e-03,  2.53660046e-02,  5.30245714e-03,\n",
       "          -2.70666108e-02, -2.84649115e-02, -7.02456087e-02,\n",
       "          -3.58996540e-02,  8.46341625e-03, -1.36347681e-01,\n",
       "          -6.17822111e-02,  1.55189157e-01, -6.13620831e-03,\n",
       "          -7.22608492e-02,  7.11962581e-02,  1.14240132e-01,\n",
       "          -6.04723431e-02,  1.29450560e-01,  2.18721014e-02,\n",
       "           7.31798708e-02]],\n",
       "\n",
       "        [[-5.74730895e-02,  3.70547697e-02, -7.79928416e-02,\n",
       "           1.16488107e-01,  8.69088992e-02,  1.07480980e-01,\n",
       "          -1.46354390e-02,  5.53230494e-02,  2.64258552e-02,\n",
       "          -1.96429957e-02, -4.94819432e-02, -6.09493852e-02,\n",
       "           8.80927742e-02,  7.82425031e-02, -2.66667277e-01,\n",
       "          -1.33021511e-02,  1.91698134e-01, -7.34848082e-02,\n",
       "          -2.28834171e-02,  1.18667081e-01,  1.39838457e-01,\n",
       "          -2.88423151e-02,  1.42747462e-01, -8.04961622e-02,\n",
       "           1.48484215e-01]],\n",
       "\n",
       "        [[ 8.50561038e-02,  1.00974381e-01,  9.02095065e-03,\n",
       "           7.12404102e-02, -3.57576758e-02, -3.31628397e-02,\n",
       "          -2.91438363e-02,  4.05511595e-02,  4.79021519e-02,\n",
       "           4.70583402e-02,  1.90759581e-02,  1.48354813e-01,\n",
       "          -1.94464490e-01,  1.78442717e-01, -2.00482443e-01,\n",
       "           4.12683226e-02, -3.36995209e-03,  3.15339752e-02,\n",
       "          -1.21021822e-01,  1.03396341e-01,  3.06485854e-02,\n",
       "          -5.17486110e-02,  1.46339104e-01, -2.32985631e-01,\n",
       "          -7.39321709e-02]],\n",
       "\n",
       "        [[-6.68570772e-02,  6.39322847e-02,  9.54841916e-03,\n",
       "          -1.70452390e-02, -8.00537094e-02,  3.11479829e-02,\n",
       "           9.31025296e-02, -8.33511725e-02, -9.01687965e-02,\n",
       "           1.24540262e-01,  3.92019264e-02,  3.69258150e-02,\n",
       "          -6.00446351e-02,  1.87291130e-01, -1.76161528e-01,\n",
       "          -2.98071206e-02, -6.10521249e-02,  1.53605295e-02,\n",
       "          -9.29236412e-02,  5.09945340e-02,  2.72987187e-02,\n",
       "           2.53022481e-02, -6.29778355e-02, -1.54784918e-01,\n",
       "          -8.42187256e-02]]],\n",
       "\n",
       "\n",
       "       [[[ 6.87564388e-02,  7.30948150e-02, -8.63095969e-02,\n",
       "           8.15833956e-02,  3.66052687e-02, -6.16180897e-03,\n",
       "           2.77597085e-03, -2.43957601e-02,  1.31785810e-01,\n",
       "          -7.40118027e-02,  6.41732989e-03, -1.36635974e-01,\n",
       "           1.18611362e-02, -2.45281570e-02,  1.82767492e-02,\n",
       "           6.13392107e-02, -4.53886837e-02, -2.99337227e-02,\n",
       "          -1.66152772e-02,  1.35419503e-01, -5.83537330e-04,\n",
       "          -9.73358452e-02,  1.15033150e-01,  1.50220692e-01,\n",
       "           1.66784972e-01]],\n",
       "\n",
       "        [[ 4.16455790e-02,  1.24657683e-01, -9.25605651e-03,\n",
       "           2.01567309e-03,  2.17276793e-02,  7.20642731e-02,\n",
       "           1.46705583e-01,  8.52946788e-02,  1.76810682e-01,\n",
       "          -6.43855939e-03, -1.09151634e-03, -5.39522804e-02,\n",
       "           1.23285405e-01,  1.33508414e-01,  2.88321711e-02,\n",
       "          -1.35035347e-02,  1.47723794e-01, -2.08227858e-02,\n",
       "           1.23551777e-02, -3.18120234e-02, -3.95255396e-03,\n",
       "          -8.48024487e-02,  2.02211458e-02,  5.74703068e-02,\n",
       "           1.21005379e-01]],\n",
       "\n",
       "        [[-4.91900928e-02, -3.24615277e-02,  1.15643695e-01,\n",
       "          -5.87128028e-02, -3.80948819e-02,  7.65372366e-02,\n",
       "          -2.26351805e-02, -2.79775001e-02,  1.44697934e-01,\n",
       "           2.62062233e-02,  3.15741971e-02,  1.61036383e-03,\n",
       "          -8.89740214e-02,  5.11224344e-02,  9.08086374e-02,\n",
       "          -1.96450148e-02,  3.70599031e-02,  7.85196349e-02,\n",
       "           1.04349189e-01, -4.10918472e-03,  1.59610912e-01,\n",
       "           1.01807512e-01,  2.09748372e-02,  4.70949756e-03,\n",
       "           1.09656923e-01]],\n",
       "\n",
       "        [[ 1.23122096e-01,  1.29847184e-01,  1.01238608e-01,\n",
       "          -1.11048803e-01,  3.12347822e-02,  1.39708415e-01,\n",
       "          -4.26423475e-02, -6.21086322e-02, -4.42651063e-02,\n",
       "           4.15664576e-02,  1.14065744e-01,  1.75038472e-01,\n",
       "          -1.16613939e-01,  1.47665292e-02, -2.57495083e-02,\n",
       "           1.77055970e-02, -4.28082459e-02, -6.05383925e-02,\n",
       "           3.52598988e-02, -5.29408567e-02,  4.96626236e-02,\n",
       "           1.12248324e-01,  1.33156747e-01, -2.65785426e-01,\n",
       "           1.18102785e-02]],\n",
       "\n",
       "        [[ 9.37261358e-02, -4.50135022e-03,  2.62438208e-02,\n",
       "          -8.90934432e-04,  5.74434251e-02,  9.67113003e-02,\n",
       "          -1.58794206e-02, -1.99222732e-02, -6.29103407e-02,\n",
       "           9.27112028e-02,  1.17034540e-01,  1.27529338e-01,\n",
       "          -1.66833356e-01,  8.92146528e-02, -2.33495813e-02,\n",
       "           4.94416170e-02, -1.60259515e-01, -3.79689932e-02,\n",
       "           1.48053974e-01,  5.30738980e-02,  3.03055104e-02,\n",
       "           4.48177308e-02,  4.23778594e-02, -1.68612033e-01,\n",
       "          -6.66641742e-02]]],\n",
       "\n",
       "\n",
       "       [[[-6.48274124e-02, -6.81869686e-02, -4.37156595e-02,\n",
       "          -1.40545711e-01, -4.45024110e-02,  1.67617295e-02,\n",
       "          -5.85338064e-02, -1.14212357e-01,  1.48445191e-02,\n",
       "           8.18229467e-02,  3.32178697e-02, -1.87255085e-01,\n",
       "           7.14725628e-02, -3.84742022e-02,  8.89433026e-02,\n",
       "           1.12593748e-01, -5.23285978e-02,  9.31878090e-02,\n",
       "           1.46073356e-01, -6.36588186e-02, -9.83976647e-02,\n",
       "           2.22465751e-04,  8.25766623e-02,  2.40699351e-01,\n",
       "           1.27422914e-01]],\n",
       "\n",
       "        [[ 3.75049971e-02,  8.47753882e-02, -4.50564101e-02,\n",
       "          -7.12874308e-02,  1.66096479e-01,  1.54254600e-01,\n",
       "           1.27583757e-01,  1.23318350e-02,  2.27825027e-02,\n",
       "           1.09716073e-01, -5.24400286e-02, -1.79614410e-01,\n",
       "           2.61274409e-02, -1.12801172e-01,  1.66103587e-01,\n",
       "           1.59382541e-02,  1.75379932e-01,  2.84285657e-03,\n",
       "           5.87212853e-02, -1.26965389e-01,  2.60656271e-02,\n",
       "           1.17664568e-01, -4.42210399e-02,  1.22670479e-01,\n",
       "          -1.25784772e-02]],\n",
       "\n",
       "        [[ 1.69140920e-01,  2.36888928e-03,  3.46674621e-02,\n",
       "          -1.88124463e-01,  1.18848763e-01,  1.84325725e-01,\n",
       "           1.46567553e-01,  7.29527101e-02,  1.52292505e-01,\n",
       "           1.89242437e-01,  1.15460828e-01,  4.04390767e-02,\n",
       "          -5.21066338e-02, -3.10870018e-02,  1.83118939e-01,\n",
       "           5.03519885e-02,  1.54934064e-01,  1.60983816e-01,\n",
       "           1.36886135e-01, -3.14554386e-02,  6.03477024e-02,\n",
       "           1.67821571e-01,  2.71172728e-02, -1.60439089e-01,\n",
       "           8.61682966e-02]],\n",
       "\n",
       "        [[ 1.82606921e-01, -6.78469986e-02,  1.93815425e-01,\n",
       "          -6.69522807e-02,  1.25481278e-01,  2.10951701e-01,\n",
       "          -1.64155348e-03,  1.09757394e-01,  1.05049126e-01,\n",
       "           1.33981764e-01,  5.54837435e-02,  1.70964748e-01,\n",
       "           1.01201490e-01, -1.14937015e-01,  1.38185948e-01,\n",
       "           1.04230389e-01, -1.75511576e-02,  1.59802988e-01,\n",
       "           7.95306340e-02, -8.47231373e-02,  1.38410047e-01,\n",
       "           6.67314976e-02, -4.01185156e-04, -1.60762757e-01,\n",
       "          -4.45103608e-02]],\n",
       "\n",
       "        [[ 6.19261637e-02, -4.70096543e-02,  8.23042914e-02,\n",
       "          -1.81336224e-01,  3.89052480e-02,  1.78646706e-02,\n",
       "          -7.59844929e-02,  1.31691262e-01, -3.63949221e-04,\n",
       "           8.81068408e-03,  7.35479072e-02,  5.71866781e-02,\n",
       "          -7.95458108e-02, -6.86426833e-02, -2.79642735e-03,\n",
       "           1.55629784e-01, -2.70892065e-02,  1.56342059e-01,\n",
       "           5.15244948e-03, -5.96224330e-02,  1.82429492e-01,\n",
       "           6.52462989e-03, -9.74118039e-02, -1.84207350e-01,\n",
       "          -4.88134064e-02]]],\n",
       "\n",
       "\n",
       "       [[[ 1.77445624e-03,  6.60065860e-02,  5.00658862e-02,\n",
       "          -1.08247504e-01,  1.39012560e-01,  8.22442919e-02,\n",
       "          -3.29688080e-02, -1.18520282e-01, -1.01953633e-01,\n",
       "           7.00053275e-02,  9.27299038e-02, -2.27832764e-01,\n",
       "           1.12387955e-01, -1.50225088e-01,  2.32638735e-02,\n",
       "          -1.17994798e-02,  9.32470337e-02, -3.24051455e-02,\n",
       "           1.02658868e-01, -7.16036931e-02, -9.13461596e-02,\n",
       "           4.10496630e-02,  5.15114814e-02,  2.36249015e-01,\n",
       "          -1.26658129e-02]],\n",
       "\n",
       "        [[ 2.60570087e-02, -1.79553535e-02, -2.58916318e-02,\n",
       "          -8.91222507e-02,  9.27950218e-02,  3.40087824e-02,\n",
       "           7.50389919e-02, -3.98560567e-03, -9.76504907e-02,\n",
       "           1.49140552e-01,  1.05664000e-01, -2.20038623e-01,\n",
       "           1.60157382e-01, -8.27581361e-02,  1.29511818e-01,\n",
       "          -1.42533602e-02, -1.34203704e-02,  1.02567762e-01,\n",
       "           1.90442219e-01, -5.02636917e-02, -1.35142326e-01,\n",
       "           1.68206971e-02, -8.81928876e-02,  2.64661089e-02,\n",
       "           8.76078531e-02]],\n",
       "\n",
       "        [[-1.12090670e-02, -2.95670368e-02,  1.35292917e-01,\n",
       "          -1.75674230e-01,  1.35864183e-01, -1.95152275e-02,\n",
       "           1.47058502e-01,  1.41174942e-01,  1.23742923e-01,\n",
       "           1.47194892e-01,  7.27781281e-02, -1.90819800e-02,\n",
       "           1.01960041e-01, -1.69654235e-01,  1.18568666e-01,\n",
       "          -5.61001971e-02,  2.63450406e-02,  1.79011315e-01,\n",
       "           1.29657900e-02, -1.22505717e-01, -8.10571387e-03,\n",
       "           1.08730480e-01, -9.30812657e-02, -5.74621260e-02,\n",
       "           8.87979940e-02]],\n",
       "\n",
       "        [[ 1.36845082e-01, -7.11592361e-02,  1.52368113e-01,\n",
       "           4.28947760e-03,  1.46465361e-01,  1.40992254e-01,\n",
       "           4.85928804e-02,  1.88088968e-01, -3.07571292e-02,\n",
       "           9.17669833e-02,  1.04454085e-01,  1.50751606e-01,\n",
       "           1.07920259e-01, -1.42235994e-01,  1.88595518e-01,\n",
       "           1.47882715e-01,  5.50014339e-02,  1.91477716e-01,\n",
       "          -4.37493064e-03, -9.87068415e-02,  1.21043511e-01,\n",
       "           1.28123045e-01,  2.82917768e-02, -2.32674107e-01,\n",
       "           3.33318971e-02]],\n",
       "\n",
       "        [[ 1.70398224e-02,  5.15457839e-02,  1.43954605e-01,\n",
       "           1.11198677e-02,  1.10352047e-01,  1.82735752e-02,\n",
       "           1.33960731e-02,  1.92390919e-01,  1.43481463e-01,\n",
       "           9.34335738e-02,  5.21668456e-02,  1.01574190e-01,\n",
       "           4.45413068e-02, -1.28862813e-01,  1.86717391e-01,\n",
       "           7.57231861e-02, -1.46663800e-01,  1.13731869e-01,\n",
       "           1.46297678e-01, -4.02248092e-02,  1.30908772e-01,\n",
       "           1.19919293e-01,  1.39691774e-02, -9.91107151e-02,\n",
       "          -1.92664694e-02]]]], dtype=float32),\n",
       "       array([ 0.00158628,  0.02013944,  0.01435409,  0.06444182,  0.02006857,\n",
       "        0.00588893,  0.00086828,  0.00439215,  0.00016384,  0.00356561,\n",
       "       -0.01588511,  0.05600903,  0.005947  ,  0.03684145,  0.00030234,\n",
       "        0.02356069, -0.02135357,  0.02363608,  0.01070969,  0.04478418,\n",
       "        0.00695596,  0.01078634,  0.05610429,  0.07275622,  0.00502416],\n",
       "      dtype=float32),\n",
       "       array([[[[-3.17800567e-02, -9.12535861e-02, -1.19838409e-01, ...,\n",
       "           1.12192765e-01, -2.47458164e-02,  9.68318135e-02],\n",
       "         [ 3.14176828e-02, -1.65319461e-02, -8.57986584e-02, ...,\n",
       "          -4.92389165e-02,  5.13577461e-02,  6.62637800e-02],\n",
       "         [ 7.38350675e-02, -6.47950545e-02, -6.41059279e-02, ...,\n",
       "          -6.29842002e-03, -4.33363691e-02,  1.14650831e-01],\n",
       "         ...,\n",
       "         [ 3.70708574e-03, -6.36905283e-02,  5.74979223e-02, ...,\n",
       "           6.64979592e-02,  7.39401877e-02, -4.08996753e-02],\n",
       "         [ 2.45520892e-03,  6.24576136e-02,  3.85356247e-02, ...,\n",
       "           2.13459246e-02,  1.85074568e-01,  4.74361219e-02],\n",
       "         [-3.47395837e-02,  1.18009478e-01,  4.56874780e-02, ...,\n",
       "          -1.30774304e-02,  2.05810461e-02,  3.54974670e-03]],\n",
       "\n",
       "        [[ 6.51817247e-02, -1.00795440e-01, -1.05257228e-01, ...,\n",
       "           1.58602558e-03, -6.07782714e-02,  6.00057505e-02],\n",
       "         [ 1.10146580e-02, -6.19143527e-03, -1.15275584e-01, ...,\n",
       "           1.08935915e-01, -5.31958081e-02, -4.56908531e-02],\n",
       "         [ 2.50760969e-02, -7.05504135e-05, -1.92636382e-02, ...,\n",
       "           1.17432110e-01,  9.05373096e-02, -7.92080611e-02],\n",
       "         ...,\n",
       "         [ 4.40099090e-02, -5.78380637e-02, -6.55790567e-02, ...,\n",
       "           1.47142773e-02, -1.18253730e-01,  4.35210876e-02],\n",
       "         [ 1.11059897e-01,  1.34521663e-01,  1.04036644e-01, ...,\n",
       "           1.79332912e-01, -1.83903445e-02,  5.57183959e-02],\n",
       "         [-9.15439948e-02, -7.57156834e-02, -8.89623463e-02, ...,\n",
       "          -3.73443961e-03, -1.30393073e-01,  5.70488349e-02]],\n",
       "\n",
       "        [[ 5.45262583e-02, -1.23507060e-01, -4.87946458e-02, ...,\n",
       "          -1.11878552e-01,  3.62819321e-02,  3.37130427e-02],\n",
       "         [-1.37189450e-02, -7.93247446e-02, -1.10364266e-01, ...,\n",
       "           9.72151607e-02,  4.09641210e-03,  1.67652164e-02],\n",
       "         [ 1.08150236e-01,  9.25259758e-03,  1.42605618e-01, ...,\n",
       "          -1.25106379e-01,  1.91090077e-01,  3.08913528e-03],\n",
       "         ...,\n",
       "         [-9.41360295e-02,  5.30517660e-03, -1.15136936e-01, ...,\n",
       "          -5.94211183e-02, -6.70713186e-03, -1.43009601e-02],\n",
       "         [ 5.19415848e-02, -1.54954595e-02, -1.50907502e-01, ...,\n",
       "           7.24065006e-02, -1.15195625e-01,  9.17861760e-02],\n",
       "         [ 1.25418529e-01, -2.76120915e-03, -1.23980485e-01, ...,\n",
       "           1.56359479e-01, -1.86426323e-02, -5.61372265e-02]]],\n",
       "\n",
       "\n",
       "       [[[-5.88371754e-02,  4.00544070e-02,  7.56472424e-02, ...,\n",
       "          -1.55122755e-02,  6.47257790e-02,  7.09104985e-02],\n",
       "         [ 4.00526710e-02,  2.12538149e-02,  6.21471703e-02, ...,\n",
       "          -8.66975486e-02,  3.83880772e-02,  9.27032307e-02],\n",
       "         [ 8.98639560e-02, -6.25191927e-02,  8.49467888e-02, ...,\n",
       "           5.66983223e-02, -1.05305716e-01, -8.97658765e-02],\n",
       "         ...,\n",
       "         [ 2.25242693e-02, -5.32856211e-02, -9.88200754e-02, ...,\n",
       "           2.80502886e-02,  5.17246090e-02,  6.89643063e-03],\n",
       "         [ 7.69857690e-02, -3.59333493e-02,  1.91736650e-02, ...,\n",
       "           1.33528933e-02,  2.06653297e-01,  3.06282658e-02],\n",
       "         [-1.08591758e-01,  4.25761752e-02, -1.07241170e-02, ...,\n",
       "           3.83833908e-02,  3.96966115e-02,  4.28299978e-02]],\n",
       "\n",
       "        [[ 3.74154896e-02,  7.91108385e-02,  7.42086321e-02, ...,\n",
       "           7.88414106e-02, -8.59911460e-03,  3.25278491e-02],\n",
       "         [ 2.67394241e-02,  4.49228138e-02,  6.11311989e-03, ...,\n",
       "           1.37601137e-01,  7.78603479e-02, -6.82912171e-02],\n",
       "         [ 3.34708691e-02, -6.31116703e-02,  2.60641463e-02, ...,\n",
       "           1.76620170e-01, -4.77452651e-02,  8.90179425e-02],\n",
       "         ...,\n",
       "         [-4.77439873e-02,  1.38587086e-02, -9.10597146e-02, ...,\n",
       "          -3.22107486e-02,  3.86476852e-02, -8.62992555e-03],\n",
       "         [-1.05005670e-02,  5.86796962e-02,  1.15346918e-02, ...,\n",
       "           1.02777749e-01, -3.18065248e-02, -2.64951345e-02],\n",
       "         [-6.20979406e-02,  5.34680150e-02, -8.86440426e-02, ...,\n",
       "           4.61529419e-02, -8.35975260e-02,  1.06940404e-01]],\n",
       "\n",
       "        [[-4.25219908e-02, -3.71431792e-03,  3.94486003e-02, ...,\n",
       "           2.76989155e-02,  7.46971741e-02, -7.76195601e-02],\n",
       "         [ 4.56399098e-03,  1.25728082e-02, -8.45523104e-02, ...,\n",
       "          -8.25213362e-03,  1.33252284e-02, -8.82158987e-03],\n",
       "         [ 1.01353019e-01, -6.74634352e-02,  5.64214811e-02, ...,\n",
       "          -1.75586000e-01,  1.10576130e-01,  3.60078663e-02],\n",
       "         ...,\n",
       "         [ 3.27364989e-02, -3.41234170e-02, -1.42913088e-01, ...,\n",
       "           8.42087790e-02,  4.92602102e-02,  5.89981526e-02],\n",
       "         [ 2.36923564e-02, -8.50400627e-02, -6.69164509e-02, ...,\n",
       "           3.92990969e-02, -1.25839666e-01,  7.33546466e-02],\n",
       "         [ 8.64962265e-02, -1.85150355e-02, -2.67521963e-02, ...,\n",
       "           6.78539053e-02, -3.33742723e-02,  1.32068574e-01]]],\n",
       "\n",
       "\n",
       "       [[[-5.11991270e-02, -3.85133885e-02, -2.71442216e-02, ...,\n",
       "          -1.94908511e-02, -9.13247988e-02,  4.50206734e-02],\n",
       "         [-9.67087448e-02,  1.93863790e-02,  3.40783969e-02, ...,\n",
       "          -9.79961380e-02,  1.09605163e-01, -8.88778642e-02],\n",
       "         [ 1.05308384e-01, -4.94371429e-02,  8.50369483e-02, ...,\n",
       "           8.67247432e-02, -7.92130157e-02, -3.60506475e-02],\n",
       "         ...,\n",
       "         [ 1.20114060e-02,  8.97174925e-02,  5.56358434e-02, ...,\n",
       "          -9.73813310e-02,  7.94218779e-02,  2.26078108e-02],\n",
       "         [-4.22513448e-02, -6.79000318e-02, -7.46893734e-02, ...,\n",
       "          -2.32928917e-01,  1.38052344e-01, -5.64929843e-02],\n",
       "         [ 5.65647595e-02,  1.01471409e-01,  1.54074957e-03, ...,\n",
       "          -1.39512524e-01,  1.17260583e-01, -8.03181231e-02]],\n",
       "\n",
       "        [[ 2.04299227e-03, -5.86742349e-02, -4.71002143e-03, ...,\n",
       "           8.44171345e-02, -1.19438253e-01,  6.02272078e-02],\n",
       "         [ 4.07499038e-02,  2.14604530e-02,  1.23049738e-02, ...,\n",
       "           7.27279484e-02, -4.44764718e-02, -5.14495857e-02],\n",
       "         [-6.64822459e-02, -1.15327232e-01,  5.59565723e-02, ...,\n",
       "           9.80056524e-02, -1.16458096e-01, -6.18980117e-02],\n",
       "         ...,\n",
       "         [-4.45491076e-02,  5.63994097e-03,  4.17080708e-02, ...,\n",
       "           3.53621580e-02, -9.36576948e-02, -8.18989985e-03],\n",
       "         [-1.91530641e-02, -5.46882823e-02, -1.89741739e-04, ...,\n",
       "           9.77579653e-02, -1.21722952e-01,  2.29108911e-02],\n",
       "         [ 5.05002849e-02,  4.03033309e-02,  2.89448332e-02, ...,\n",
       "           4.52343747e-03, -1.80556215e-02,  2.29870938e-02]],\n",
       "\n",
       "        [[ 2.48350315e-02,  2.60579977e-02,  9.22021922e-03, ...,\n",
       "          -5.55800498e-02, -2.82960720e-02,  8.20151940e-02],\n",
       "         [ 7.82815441e-02,  7.91173875e-02, -2.95031555e-02, ...,\n",
       "           7.71699175e-02,  1.39480382e-02,  2.08469480e-02],\n",
       "         [-2.93798801e-02,  3.56820412e-02, -1.02488615e-03, ...,\n",
       "          -2.21864926e-03,  1.32412508e-01,  2.70092934e-02],\n",
       "         ...,\n",
       "         [ 1.06190078e-01,  1.01257868e-01, -1.24875521e-02, ...,\n",
       "          -2.41200775e-02, -5.29635064e-02, -5.05115353e-02],\n",
       "         [ 5.20908684e-02,  2.03735400e-02,  4.31732535e-02, ...,\n",
       "           2.81411894e-02, -6.28717914e-02, -4.06584479e-02],\n",
       "         [ 1.74947479e-03, -8.66902061e-03,  8.95814821e-02, ...,\n",
       "           5.19132465e-02, -4.88216132e-02,  1.02150314e-01]]]],\n",
       "      dtype=float32),\n",
       "       array([ 0.00931932,  0.00858671, -0.00255811,  0.04944756, -0.00898191,\n",
       "        0.02198988, -0.00462291,  0.00564401,  0.00987841,  0.00066065,\n",
       "        0.01412699, -0.0081571 ,  0.01311403,  0.00260767,  0.01180754,\n",
       "        0.01094261,  0.02513718,  0.02529363,  0.01244265,  0.02252703,\n",
       "        0.00232951,  0.00618248,  0.00539126,  0.03276532,  0.04305966,\n",
       "       -0.0039453 ,  0.03827798,  0.01800532,  0.03984033, -0.02387842,\n",
       "        0.02915092,  0.00935749, -0.01785425,  0.01154611,  0.00324078,\n",
       "       -0.00257727,  0.00115028, -0.00419966,  0.02663569,  0.01153039,\n",
       "        0.02007727,  0.01451497,  0.01764455,  0.03002856,  0.00885432,\n",
       "        0.00454808,  0.00585946,  0.00549786,  0.03003348, -0.00644222],\n",
       "      dtype=float32),\n",
       "       array([[ 5.42588823e-05, -3.33055411e-03, -6.97242562e-03, ...,\n",
       "        -2.65291389e-02, -1.96278710e-02, -2.25601383e-02],\n",
       "       [ 3.85445310e-05, -2.22400744e-02,  5.05944304e-02, ...,\n",
       "        -3.63289528e-02,  6.18036313e-04, -3.37627195e-02],\n",
       "       [ 8.14400410e-05, -1.97321177e-02,  2.26416122e-02, ...,\n",
       "        -5.65023683e-02,  1.52897779e-02, -3.98860574e-02],\n",
       "       ...,\n",
       "       [ 1.21730554e-04,  2.81165373e-02,  2.64078211e-02, ...,\n",
       "         9.40628275e-02, -2.96573844e-02, -9.61252078e-02],\n",
       "       [-2.23808584e-05,  1.59532223e-02,  7.60890171e-03, ...,\n",
       "         7.82148466e-02,  4.50948998e-02, -3.29601741e-03],\n",
       "       [ 5.28624041e-05,  4.76852506e-02, -2.06118301e-02, ...,\n",
       "         3.71300615e-02, -7.94379562e-02, -4.25816700e-02]], dtype=float32),\n",
       "       array([-1.53227802e-03,  1.49343088e-02,  1.20174373e-02,  1.29577797e-03,\n",
       "        9.05582402e-03,  9.17172339e-03, -3.28533445e-03,  6.40158355e-03,\n",
       "        2.06258819e-02,  1.79313049e-02,  1.29978703e-02,  1.69287194e-02,\n",
       "        8.40285607e-03, -6.28842157e-04, -3.24978726e-03,  7.95632694e-03,\n",
       "        3.38668656e-03,  1.83128268e-02, -2.09579547e-03, -1.33420026e-03,\n",
       "        1.35571184e-02,  1.65627953e-02,  6.56852080e-03, -1.85133284e-03,\n",
       "       -2.13701860e-03,  1.86756917e-03,  4.29172022e-03,  2.29592528e-02,\n",
       "       -1.41418097e-03,  3.52358050e-03, -1.61643047e-03,  8.72748438e-03,\n",
       "        1.68945864e-02,  1.53351587e-03,  2.06792206e-02, -1.80395495e-03,\n",
       "        2.07266659e-02,  1.62971150e-02,  1.06359543e-02,  1.86687578e-02,\n",
       "        1.69345997e-02, -8.47505592e-03, -7.42340088e-03,  1.12918355e-02,\n",
       "        1.17715085e-02,  8.47990066e-03,  8.80655646e-03,  9.62544791e-03,\n",
       "        1.83742009e-02, -6.28003152e-04,  2.08130735e-03,  7.12957606e-03,\n",
       "        2.70731188e-03, -1.56611507e-03,  1.93044003e-02,  6.99488958e-03,\n",
       "        1.38419392e-02,  2.04016641e-02,  5.19143534e-04,  1.28277931e-02,\n",
       "        9.04783793e-03, -5.93483960e-03, -2.01317715e-03,  9.49454121e-03,\n",
       "        4.64038225e-04, -1.41363576e-04,  8.15836887e-04,  3.61013692e-03,\n",
       "       -1.46887702e-04,  8.68794974e-03,  1.58928409e-02,  1.44143235e-02,\n",
       "       -4.46153397e-04, -7.28992047e-04,  1.10073416e-02,  1.14674103e-02,\n",
       "        2.95166252e-03,  9.63689573e-03,  4.14012372e-03, -2.49240920e-03,\n",
       "        5.78016788e-03,  6.95547881e-03,  6.87692408e-03,  6.93546655e-03,\n",
       "        1.32194003e-02,  1.28352772e-02, -1.20217132e-03,  1.70685146e-02,\n",
       "       -5.57677471e-04,  2.85839033e-03, -2.48151482e-03,  2.13046893e-02,\n",
       "       -8.33407452e-04,  1.79424393e-03,  3.77453608e-03, -2.89168023e-03,\n",
       "        1.67878773e-02,  1.07091844e-04,  9.73269716e-03,  1.20658698e-02,\n",
       "       -2.10150387e-02,  1.30442092e-02,  1.11760180e-02, -1.02016469e-03,\n",
       "        1.35442754e-02,  5.02913119e-03, -3.32078431e-04,  4.25338699e-03,\n",
       "        2.19099633e-02,  1.84013823e-03,  1.38654755e-02, -5.26943570e-03,\n",
       "        1.74687710e-02,  1.83128286e-02,  9.13907681e-03,  8.58790241e-03,\n",
       "       -5.78718120e-03,  1.03878370e-02, -4.23454959e-03,  1.59107503e-02,\n",
       "        1.42366653e-02,  1.05272224e-02,  9.73566528e-03, -7.89738435e-04,\n",
       "        6.10017637e-03,  5.80064161e-03,  1.21821975e-02,  7.56648742e-03,\n",
       "        7.75476266e-03, -2.60717608e-03, -2.90786847e-03,  1.28817614e-02,\n",
       "        1.95024684e-02,  1.97413489e-02,  1.30640520e-02,  1.10287862e-02,\n",
       "        2.07762187e-03,  1.49604892e-02,  1.81730036e-02,  1.94726523e-03,\n",
       "       -2.35144212e-03, -2.19402416e-03,  3.85688245e-03, -1.74557220e-03,\n",
       "        3.86824133e-04,  1.75553262e-02, -3.54100065e-03,  1.42243402e-02,\n",
       "        7.87911750e-03,  1.21333506e-02,  4.00845446e-02,  3.59473657e-03,\n",
       "        2.66386964e-03, -1.78499054e-02,  9.59012378e-03,  1.36391195e-02,\n",
       "       -3.40949628e-04,  6.80853333e-03,  1.29009206e-02, -2.12647277e-03,\n",
       "        1.22378590e-02, -5.16194431e-03,  8.11117701e-03, -6.42982253e-04,\n",
       "        9.41416249e-03,  4.62057535e-03,  4.11596755e-03,  5.24462573e-03,\n",
       "        6.00353756e-04,  2.29305937e-03, -3.30611889e-04,  1.46541139e-03,\n",
       "        1.54845677e-02,  5.77236060e-03,  6.89726928e-03,  2.51167212e-02,\n",
       "       -1.95126201e-03,  4.96787159e-03,  1.10228732e-03,  8.86513107e-03,\n",
       "        6.07985910e-03,  4.85729938e-03,  1.41783115e-02, -8.59955885e-03,\n",
       "        1.76193677e-02,  1.07838521e-02,  1.65698770e-02,  7.79027166e-03,\n",
       "        1.66357365e-02,  2.23029740e-02,  1.22338044e-03,  7.15872739e-03,\n",
       "        1.22437708e-03,  1.73069909e-02,  2.06199959e-02,  1.33446406e-03,\n",
       "       -1.45849423e-03,  1.04499767e-02,  6.53386349e-03, -2.41490267e-03,\n",
       "       -3.37913493e-03,  2.00969335e-02,  8.48933402e-03,  5.25678461e-03,\n",
       "        4.29913984e-04,  2.00784095e-02,  3.77051113e-03,  6.68299524e-03,\n",
       "        2.03924477e-02, -5.97455632e-03,  3.47349653e-03,  9.91619471e-03,\n",
       "        2.90802913e-04, -2.67811562e-03,  1.23106632e-02, -1.37386126e-02,\n",
       "        1.09744985e-02, -2.71296507e-04,  2.94758729e-03,  1.61267146e-02,\n",
       "        4.82876599e-03,  6.29170099e-03,  6.96539786e-03,  9.70943191e-04,\n",
       "        2.56561185e-03,  8.70330376e-04, -3.17192404e-03,  1.64837241e-02,\n",
       "        5.13461698e-03,  1.05383880e-02,  6.94370596e-03,  1.52559839e-02,\n",
       "        1.74713973e-02,  6.69155689e-03,  1.15402220e-02,  1.63506158e-02,\n",
       "        1.27221607e-02,  5.11567155e-03,  1.66523438e-02, -3.09208478e-03,\n",
       "        6.28658943e-03,  6.87320717e-03,  2.20973603e-02,  2.87634251e-03,\n",
       "        4.99527669e-03, -1.65040034e-03,  1.35733364e-02,  1.46719897e-02,\n",
       "       -2.44299648e-03,  1.50704989e-02,  1.58626717e-02, -1.47151724e-02,\n",
       "        1.02961212e-02,  3.71312210e-03,  9.85299982e-03, -1.24938309e-03,\n",
       "        1.10519398e-02,  8.61353520e-03, -1.43009145e-03,  2.52103750e-02,\n",
       "       -9.21379868e-03, -1.54984975e-03, -1.31494785e-03,  7.45354872e-03,\n",
       "        1.37671158e-02,  1.00556808e-02,  8.27704649e-03,  1.48561252e-02,\n",
       "        1.89683307e-02,  9.30347294e-03,  6.00563083e-03,  1.77402478e-02,\n",
       "        5.90688922e-03,  9.75922588e-03,  8.41310993e-03,  5.37325628e-03,\n",
       "        7.04385480e-03,  5.22834389e-03,  9.67725739e-03, -2.29863985e-03,\n",
       "        3.91708268e-03,  1.36496162e-03, -3.39175598e-03, -1.37313912e-02,\n",
       "        5.68409450e-03,  1.01574538e-02,  4.37609991e-03,  1.01807155e-02,\n",
       "        8.15860834e-03,  1.51176760e-02,  1.44466599e-02, -1.33885548e-03,\n",
       "        6.86965650e-03,  1.31834634e-02,  1.49080425e-03,  8.14051554e-03,\n",
       "        6.66864589e-03,  1.47787258e-02,  9.94522125e-03, -2.96783005e-03,\n",
       "       -1.72569184e-03,  1.45435715e-02,  7.46756268e-04,  1.08177569e-02,\n",
       "        9.83653031e-03,  2.90064863e-03,  1.83878355e-02,  1.52318785e-02,\n",
       "        1.17921177e-02, -2.66924326e-05, -2.56988709e-03,  6.67902315e-03,\n",
       "        2.09635845e-03,  5.78164961e-03,  2.87985289e-03,  7.96664786e-03,\n",
       "        8.58489517e-03,  1.13602942e-02,  1.21437162e-02,  8.11107829e-03,\n",
       "        7.80836493e-03, -2.98900210e-04,  3.86675703e-03, -3.55218025e-03,\n",
       "       -7.62481801e-03,  1.28458068e-02,  9.76037700e-03,  1.19229173e-02,\n",
       "       -3.98312043e-03, -2.79614935e-03,  2.16036923e-02,  1.92550756e-02,\n",
       "       -9.99806914e-04,  1.34927677e-02,  1.30059309e-02, -3.79361300e-04,\n",
       "        1.41169932e-02,  1.93417668e-02, -6.77735312e-04,  1.98173262e-02,\n",
       "       -6.30392134e-03,  7.13407248e-03,  1.08228587e-02,  3.11986706e-03,\n",
       "        1.48178879e-02, -6.90143497e-04, -8.23351648e-03,  4.35487786e-03,\n",
       "        3.95592162e-03,  3.45633039e-03,  1.14821037e-02, -1.31470682e-02,\n",
       "        9.84747522e-03,  1.74243364e-03,  2.94967787e-03, -3.71190516e-04,\n",
       "        5.43590402e-03,  3.81115661e-03,  9.54190083e-03,  1.31895579e-02,\n",
       "        1.55340340e-02,  1.00593537e-03, -1.86660339e-03,  2.77640997e-04,\n",
       "        8.92178435e-03,  7.28757167e-03,  7.46854069e-03,  9.88604873e-03,\n",
       "        2.38513444e-02,  1.36225773e-02,  1.22477636e-02,  2.40265876e-02,\n",
       "       -2.28851242e-03,  2.17881948e-02,  1.37127787e-02,  9.13191214e-03,\n",
       "       -5.82585577e-04, -2.10271147e-03,  5.07582678e-03,  1.36413956e-02,\n",
       "        1.05193621e-02,  1.84511896e-02,  1.47286505e-02,  1.10902935e-02,\n",
       "        1.57577060e-02,  8.62161722e-03, -1.57154202e-02,  9.69561748e-03,\n",
       "        6.46645529e-03,  5.45167318e-03,  2.45973226e-02, -1.49988467e-02,\n",
       "        1.00767706e-02,  2.10187398e-03,  9.32622422e-03,  8.85485578e-03,\n",
       "        7.61723379e-03, -2.74150807e-04, -3.02304607e-03,  7.47515028e-03,\n",
       "        3.97214806e-03,  7.65041821e-03,  7.89223181e-04,  1.02597894e-02,\n",
       "        2.03634780e-02,  1.21367769e-02,  1.00284759e-02,  1.52193876e-02,\n",
       "        1.33591294e-02,  3.55723035e-03,  7.27149891e-03,  1.50032891e-02,\n",
       "        3.61004565e-03, -3.46172019e-04,  9.47743375e-03,  1.89168081e-02,\n",
       "        1.64614487e-02,  9.74947494e-03,  5.28896088e-03,  8.01044423e-03,\n",
       "        1.53291104e-02,  5.22596436e-03,  1.63596682e-02,  1.84685420e-02,\n",
       "        3.71364132e-03, -1.11126411e-03, -4.67464095e-03,  5.25006698e-03,\n",
       "        1.41412299e-02,  1.75977070e-02,  1.35372030e-02,  2.39277028e-06,\n",
       "       -1.79461553e-04, -2.09012581e-03, -6.79594185e-03,  8.15203227e-03,\n",
       "       -2.71224362e-05,  1.29415356e-02,  1.12161795e-02, -6.37585472e-04,\n",
       "        1.40584679e-03,  1.01841893e-02,  3.85648664e-03,  1.05725816e-02,\n",
       "        1.66540705e-02,  1.30387759e-02,  1.97853800e-02,  5.54072624e-03,\n",
       "        3.41406069e-03, -2.33927532e-03,  3.10298172e-03,  6.17389381e-03,\n",
       "        1.67258121e-02,  1.46468645e-02,  1.17823090e-02,  1.53516000e-02,\n",
       "       -1.32885156e-03,  1.10232113e-02,  1.39302518e-02,  7.16157304e-03,\n",
       "        3.93435685e-03,  1.56293437e-02, -5.51960547e-04,  1.53778596e-02,\n",
       "        1.36754652e-02,  8.28893203e-03,  6.03945460e-03,  1.80829521e-02,\n",
       "        6.15019165e-03, -9.44299530e-03,  1.31793423e-02,  1.03912819e-02,\n",
       "        4.15586773e-03,  1.14234732e-02,  8.06508865e-03,  5.41940564e-03,\n",
       "        1.51409023e-02,  2.19811406e-02,  9.31939576e-03,  1.26929199e-02,\n",
       "        5.42705460e-03,  6.05178857e-03,  3.80939548e-03, -1.87489524e-04,\n",
       "        5.88721875e-03,  1.57985289e-03,  6.81494409e-03,  5.76634920e-05,\n",
       "        9.35401767e-03,  3.84770869e-03,  5.47124166e-03, -9.28812195e-03,\n",
       "        1.97718106e-02,  2.13794895e-02,  2.29782127e-02,  2.34289207e-02,\n",
       "       -4.30776505e-04,  1.81964189e-02,  1.43579429e-03,  8.38220119e-03],\n",
       "      dtype=float32),\n",
       "       array([[-0.00631948,  0.00170485,  0.01571352, ...,  0.00969293,\n",
       "        -0.00236739,  0.00045561],\n",
       "       [-0.0231467 , -0.01754107, -0.06456718, ..., -0.01910509,\n",
       "        -0.11394239,  0.14444622],\n",
       "       [-0.11336395,  0.09152488,  0.03463809, ...,  0.03281999,\n",
       "         0.11129479, -0.06267034],\n",
       "       ...,\n",
       "       [-0.04475593, -0.09292264,  0.02035329, ..., -0.09356349,\n",
       "        -0.01028317,  0.09516186],\n",
       "       [ 0.08562976,  0.11292332,  0.0362464 , ...,  0.1133973 ,\n",
       "        -0.00406671, -0.22445002],\n",
       "       [ 0.07339247,  0.08088122,  0.1373197 , ..., -0.09722737,\n",
       "        -0.12365291, -0.02838616]], dtype=float32),\n",
       "       array([-0.01533   , -0.00243436,  0.00397358, -0.00232454, -0.00385317,\n",
       "       -0.00281428, -0.0124685 ,  0.00073895,  0.01549627,  0.00190365],\n",
       "      dtype=float32),\n",
       "       array([-0.58518595, -0.49635497, -0.40658337, -0.31674835, -0.22354259,\n",
       "       -0.12078577, -0.03175697,  0.00228331,  0.03226547,  0.12020566,\n",
       "        0.22274101,  0.31652054,  0.4062971 ,  0.49612236,  0.5850084 ],\n",
       "      dtype=float32),\n",
       "       array([3.9573228, 4.0401053, 4.080157 , 4.096653 , 4.0477033, 3.9363368,\n",
       "       3.9563596, 4.1454673, 4.1520057, 4.1511164, 3.970095 , 3.9309955,\n",
       "       4.0446315, 4.0958815, 4.081266 , 4.0414357], dtype=float32),\n",
       "       array([-8.362596 , -8.399838 , -8.475231 , -8.644859 , -8.285697 ,\n",
       "       -6.69961  , -5.985868 , -5.965625 , -5.974473 , -6.623961 ,\n",
       "       -8.230631 , -8.653989 , -8.4785595, -8.401545 , -8.363429 ],\n",
       "      dtype=float32)], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of model is: \n",
      "\n",
      "Reference Network: 0.9740 \n",
      "\n",
      "Re-trained Network: 0.9760 \n",
      "\n",
      "Post Processed Network: 0.9442 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of model is: \\n\")\n",
    "\n",
    "acc = pre_trained_model.evaluate({'input':X_test,}, {\"error_loss\": Y_test,}, verbose=0)[1]\n",
    "print(\"Reference Network: %.4f \\n\" % acc)\n",
    "\n",
    "acc2 = compressed_model.evaluate({'input': X_test,}, {\"error_loss\": Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Re-trained Network: %.4f \\n\" % acc2)\n",
    "\n",
    "compressed_model.set_weights(weights_compressed)\n",
    "\n",
    "acc3 = compressed_model.evaluate({'input': X_test,}, {\"error_loss\": Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Post Processed Network: %.4f \\n\" % acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check the number of weights that were pruned, we do the following procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Non-Zero Weights: 12.454 %\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import special_flatten as flatten_1\n",
    "weight_vec = flatten_1(weights_compressed[:-3]).flatten()\n",
    "print(\"Percentage of Non-Zero Weights: %.3f %%\" % (1 - (np.count_nonzero(weight_vec)/ weight_vec.size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import save_histogram\n",
    "save_histogram(pre_trained_model.get_weights(), save=\"Figures/reference\")\n",
    "save_histogram(weights_retrain, save=\"Figures/retrain\")\n",
    "save_histogram(weights_compressed, save=\"Figures/Post-Processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Resultant Images and Discussions\n",
    "\n",
    "|Weight distribution before retraining | Weight distribution after retraining|  Weight distribution after post-processing  \n",
    ":-------------------------:|:-------------------------:|:------------:|:-------------------------:\n",
    "histogram|![](./Figures/reference.png)|<img src=\"./Figures/retrain.png\"/>| ![](./Figures/Post-Processing.png)\n",
    "log-scaled histogram|![](./Figures/reference_log.png)|<img src=\"./Figures/retrain_log.png\"/>| ![](./Figures/Post-Processing_log.png)\n",
    "\n",
    "The histograms above supports the claims of the paper. We have shown the results for the histograms and log-scaled histograms. Here, the difference between the weight distributions before and after retraining is evident from the first two columns. Similarly, in the third column i.e. after performing Post-Processing, we can see that the non-zero weights are much less as compared to the original model. We also calculate the percentage of non-zero weights and it's only 10% of the orignal number of components which demonstrates the compression of neural network to a good extent. We are also running our optimizer and train the network for 50 epochs, it takes a bit of time because of large number of parameters. \n",
    "All the images attached here can be found in the Figures Folder of the submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Karen Ullrich, Edward Meeds, and Max Welling. Soft Weight-Sharing for Neural Network Compression 2017. eprint:arXiv:1702.04008. url: https://arxiv.org/abs/1702.04008\n",
    "\n",
    "[2] [Keras Tutorial on CNNs](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py)\n",
    "\n",
    "[3] Multiple codes from Keras.io - https://keras.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
