{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing neural networks with Gaussian mixture priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "In this project we have implemented the paper titled [Soft weight-sharing for Neural Network compression](https://arxiv.org/abs/1702.04008) by Ullrich, Meeds and Welling. The main idea of the paper is to introduce a prior on the weights of a pre-trained network that encourages a lot of weights to go to zero and clusters the remaining points around a small number of discrete value. \n",
    "\n",
    "This is done by using a Gaussian Mixture prior over the weights such that the most of the weights map to a gaussian with zero mean and the rest of the weights are quantized to their closest cluster centers.\n",
    "From the paper:\n",
    "> By fitting the mixture components alongside the weights, the weights tend to concentrate very tightly around a number of cluster components, while the cluster centers optimize themselves to give the network high predictive accuracy. Compression is achieved because we only need to encode K cluster means (in full precision) in addition to the assignment of each weight to one of these J values (using log(J) bits per weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of method\n",
    "Following are the steps to achieve compression using the methods described in the given paper:\n",
    "1. Retraining a pre trained network with gaussian mixture prior on the weights\n",
    "2. Clustering the weights, merging redundant components and retrain\n",
    "3. Quantize the weights by mapping them to nearest cluster mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/archit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from data import get_mnist\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,  Activation, Flatten, Conv2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 25)        650       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 5, 50)          11300     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               625500    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "_________________________________________________________________\n",
      "error_loss (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 642,460.0\n",
      "Trainable params: 642,460.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the training data, this loads the mnist dataset if not already present\n",
    "X_train, X_test, Y_train, Y_test, img_rows, img_cols, num_classes = get_mnist()\n",
    "\n",
    "# Create a data input layer\n",
    "InputLayer = Input(shape=(img_rows, img_cols,1), name=\"input\")\n",
    "\n",
    "# First convolution layer\n",
    "conv_1 = Conv2D(25, (5, 5), strides = (2,2), activation = \"relu\")(InputLayer)\n",
    "# Second convolution layer\n",
    "conv_2 = Conv2D(50, (3, 3), strides = (2,2), activation = \"relu\")(conv_1)\n",
    "\n",
    "# 2 fully connected layers with RELU activations\n",
    "conv_output = Flatten()(conv_2)\n",
    "fc1 = Dense(500)(conv_output)\n",
    "fc1 = Activation(\"relu\")(fc1)\n",
    "fc2 = Dense(num_classes)(fc1)\n",
    "PredictionLayer = Activation(\"softmax\", name =\"error_loss\")(fc2)\n",
    "\n",
    "# Fianlly, we create a model object:\n",
    "model = Model(inputs=[InputLayer], outputs=[PredictionLayer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2550: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 15s - loss: 0.2881 - acc: 0.9216 - val_loss: 0.0969 - val_acc: 0.9692\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 15s - loss: 0.0728 - acc: 0.9780 - val_loss: 0.0481 - val_acc: 0.9855\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 14s - loss: 0.0468 - acc: 0.9858 - val_loss: 0.0410 - val_acc: 0.9862\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 14s - loss: 0.0358 - acc: 0.9890 - val_loss: 0.0357 - val_acc: 0.9890\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 14s - loss: 0.0274 - acc: 0.9916 - val_loss: 0.0385 - val_acc: 0.9872\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 14s - loss: 0.0219 - acc: 0.9932 - val_loss: 0.0370 - val_acc: 0.9880\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 14s - loss: 0.0163 - acc: 0.9949 - val_loss: 0.0309 - val_acc: 0.9906\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 14s - loss: 0.0124 - acc: 0.9961 - val_loss: 0.0361 - val_acc: 0.9880\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 14s - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0390 - val_acc: 0.9881\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 15s - loss: 0.0085 - acc: 0.9975 - val_loss: 0.0412 - val_acc: 0.9891\n",
      "Test accuracy: 0.9891\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer, loss = {\"error_loss\": \"categorical_crossentropy\"}, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=X_train, y=Y_train, \n",
    "          epochs= epochs, batch_size = batch_size,\n",
    "          verbose = 1, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model, \"./my_pretrained_net\")\n",
    "\n",
    "# model = keras.models.load_model(\"./my_pretrained_net\")\n",
    "pre_trained_model = keras.models.load_model(\"./my_pretrained_net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1026: calling reduce_min (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/archit/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1008: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 12, 12, 25)    650                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 5, 5, 50)      11300                                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1250)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 500)           625500                                       \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            5010                                         \n",
      "____________________________________________________________________________________________________\n",
      "error_loss (Activation)          (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "complexity_loss (GMMPrior)       (None, 1)             46                                           \n",
      "====================================================================================================\n",
      "Total params: 642,506.0\n",
      "Trainable params: 642,506.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from priors import GMMPrior\n",
    "from keras_helpers import fetch_weights\n",
    "\n",
    "pi_zero = 0.099\n",
    "\n",
    "reg_layer = GMMPrior(16, fetch_weights(model), pre_trained_model.get_weights(), pi_zero, name=\"complexity_loss\")(fc2)\n",
    "\n",
    "model = Model(inputs=[InputLayer], outputs=[PredictionLayer, reg_layer])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimizers \n",
    "from keras_helpers import identity\n",
    "\n",
    "tau = 0.003\n",
    "N = X_train.shape[0] \n",
    "\n",
    "opt = optimizers.Adam(lr = [5e-4,1e-4,3e-3,3e-3],  #[unnamed, means, log(precition), log(mixing proportions)]\n",
    "                      param_types_dict = ['means','gammas','rhos'])\n",
    "\n",
    "model.compile(optimizer = opt,\n",
    "              loss = {\"error_loss\": \"categorical_crossentropy\", \"complexity_loss\": identity},\n",
    "              loss_weights = {\"error_loss\": 1. , \"complexity_loss\": tau/N},\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 166s - loss: 0.0034 - error_loss_loss: 0.0044 - complexity_loss_loss: -20698.2130 - error_loss_acc: 0.9988 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 165s - loss: -0.0078 - error_loss_loss: 0.0026 - complexity_loss_loss: -208725.1393 - error_loss_acc: 0.9994 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 166s - loss: -0.0174 - error_loss_loss: 0.0021 - complexity_loss_loss: -388581.5026 - error_loss_acc: 0.9995 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 166s - loss: -0.0265 - error_loss_loss: 0.0016 - complexity_loss_loss: -562559.5417 - error_loss_acc: 0.9997 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 167s - loss: -0.0349 - error_loss_loss: 0.0013 - complexity_loss_loss: -723138.5411 - error_loss_acc: 0.9998 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 167s - loss: -0.0416 - error_loss_loss: 9.7178e-04 - complexity_loss_loss: -852118.8271 - error_loss_acc: 0.9999 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 167s - loss: -0.0457 - error_loss_loss: 0.0016 - complexity_loss_loss: -947809.6794 - error_loss_acc: 0.9996 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 168s - loss: -0.0483 - error_loss_loss: 0.0026 - complexity_loss_loss: -1017067.8746 - error_loss_acc: 0.9993 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 167s - loss: -0.0521 - error_loss_loss: 0.0014 - complexity_loss_loss: -1068988.8032 - error_loss_acc: 0.9998 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 168s - loss: -0.0548 - error_loss_loss: 0.0010 - complexity_loss_loss: -1116915.8459 - error_loss_acc: 0.9999 - complexity_loss_acc: 0.0000e+00     \n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 167s - loss: -0.0551 - error_loss_loss: 0.0029 - complexity_loss_loss: -1159519.6530 - error_loss_acc: 0.9992 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 167s - loss: -0.0552 - error_loss_loss: 0.0043 - complexity_loss_loss: -1188813.6446 - error_loss_acc: 0.9988 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 167s - loss: -0.0575 - error_loss_loss: 0.0033 - complexity_loss_loss: -1216651.0041 - error_loss_acc: 0.9992 - complexity_loss_acc: 0.0000e+00   \n",
      "Epoch 14/20\n",
      "36864/60000 [=================>............] - ETA: 64s - loss: -0.0600 - error_loss_loss: 0.0023 - complexity_loss_loss: -1245310.2431 - error_loss_acc: 0.9995 - complexity_loss_acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 512\n",
    "model.fit({\"input\": X_train,},\n",
    "          {\"error_loss\" : Y_train, \"complexity_loss\": np.zeros((N,1))},\n",
    "          epochs = epochs,\n",
    "          batch_size = batch_size,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3 - Post Processing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_retrain = np.copy(model.get_weights())\n",
    "weights_compressed = np.copy(model.get_weights())\n",
    "weights_compressed[:-3] = helper_functions.discretesize(np.copy(weights_compressed), pi_zero = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next step is to compare the accuracy of the pre-trained network with the network obtained post-processing. The procedure to do that is as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tytyiti\n",
    "model.get_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of model is: \\n\")\n",
    "\n",
    "acc = pre_trained_model.evaluate({'input':X_test,}, {\"error_loss\": Y_test,}, verbose=0)[1]\n",
    "print(\"Reference Network: %.4f \\n\" % acc)\n",
    "\n",
    "acc2 = model.evaluate({'input': X_test,}, {\"error_loss\": Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Re-trained Network: %.4f \\n\" % acc2)\n",
    "\n",
    "model.set_weights(weights_compressed)\n",
    "\n",
    "acc3 = model.evaluate({'input': X_test,}, {\"error_loss\": Y_test, \"complexity_loss\": Y_test,}, verbose=0)[3]\n",
    "print(\"Post Processed Network: %.4f \\n\" % 1 - acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check the number of weights that were pruned, we do the following procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import special_flatten as flatten_1\n",
    "weight_vec = flatten_1(weights_compressed[:-3]).flatten()\n",
    "print(\"Percentage of Non-Zero Weights: %.3f %%\" % (100.* (1 - np.count_nonzero(weight_vec)/ weight_vec.size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import save_histogram\n",
    "save_histogram(pre_trained_model.get_weights(), save=\"Figures/reference\")\n",
    "save_histogram(weights_retrain, save=\"Figures/retrain\")\n",
    "save_histogram(weights_compressed, save=\"Figures/Post-Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
